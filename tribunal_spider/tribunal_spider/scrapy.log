2017-03-23 10:48:02 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 10:48:02 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 10:48:49 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 10:48:49 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 10:51:12 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 10:51:12 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 10:51:38 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 10:51:38 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 10:51:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 10:51:38 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 10:51:38 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 10:51:38 [twisted] CRITICAL: Unhandled error in Deferred:
2017-03-23 10:51:38 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1299, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python3.6/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/usr/local/lib/python3.6/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "/usr/local/lib/python3.6/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/usr/local/lib/python3.6/site-packages/scrapy/middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/misc.py", line 44, in load_object
    mod = import_module(module)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 978, in _gcd_import
  File "<frozen importlib._bootstrap>", line 961, in _find_and_load
  File "<frozen importlib._bootstrap>", line 950, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 655, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 205, in _call_with_frames_removed
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/midle.py", line 7, in <module>
    import arbitration_spider.Environmental_parameters as Environmental_parameters
ModuleNotFoundError: No module named 'arbitration_spider.Environmental_parameters'
2017-03-23 10:54:04 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 10:54:04 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 10:54:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 10:54:04 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 10:54:04 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 10:54:04 [twisted] CRITICAL: Unhandled error in Deferred:
2017-03-23 10:54:04 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1299, in _inlineCallbacks
    result = g.send(result)
  File "/usr/local/lib/python3.6/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/usr/local/lib/python3.6/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/engine.py", line 69, in __init__
    self.downloader = downloader_cls(crawler)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/__init__.py", line 88, in __init__
    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)
  File "/usr/local/lib/python3.6/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/usr/local/lib/python3.6/site-packages/scrapy/middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/misc.py", line 44, in load_object
    mod = import_module(module)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 978, in _gcd_import
  File "<frozen importlib._bootstrap>", line 961, in _find_and_load
  File "<frozen importlib._bootstrap>", line 950, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 655, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 678, in exec_module
  File "<frozen importlib._bootstrap>", line 205, in _call_with_frames_removed
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/midle.py", line 7, in <module>
    import arbitration_spider.Environmental_parameters as Environmental_parameters
ModuleNotFoundError: No module named 'arbitration_spider.Environmental_parameters'
2017-03-23 10:54:57 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 10:54:57 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 10:54:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 10:54:57 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 10:54:57 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 10:54:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 10:54:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 10:54:57 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 10:54:57 [scrapy.core.engine] INFO: Spider opened
2017-03-23 10:54:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 10:55:00 [zhejiang] ERROR: save_time_error:time data '2017-04-24 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:00 [zhejiang] ERROR: save_time_error:time data '2017-04-21 08' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:00 [zhejiang] ERROR: save_time_error:time data '2017-04-24 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:00 [zhejiang] ERROR: save_time_error:time data '2017-04-24 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:01 [zhejiang] ERROR: save_time_error:time data '2017-04-21 10' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:01 [zhejiang] ERROR: save_time_error:time data '2017-04-21 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:01 [zhejiang] ERROR: save_time_error:time data '2017-04-25 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:01 [zhejiang] ERROR: save_time_error:time data '2017-04-25 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:01 [zhejiang] ERROR: save_time_error:time data '2017-04-26 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:01 [zhejiang] ERROR: save_time_error:time data '2017-04-26 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:01 [zhejiang] ERROR: save_time_error:time data '2017-04-25 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:01 [zhejiang] ERROR: save_time_error:time data '2017-04-24 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:01 [zhejiang] ERROR: save_time_error:time data '2017-04-25 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:01 [zhejiang] ERROR: save_time_error:time data '2017-04-26 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:02 [zhejiang] ERROR: save_time_error:time data '2016-06-01 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:02 [zhejiang] ERROR: save_time_error:time data '2017-04-27 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:02 [zhejiang] ERROR: save_time_error:time data '2017-04-27 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:02 [zhejiang] ERROR: save_time_error:time data '2015-12-21 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:02 [zhejiang] ERROR: save_time_error:time data '2015-09-21 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:02 [zhejiang] ERROR: save_time_error:time data '2015-10-27 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:02 [zhejiang] ERROR: save_time_error:time data '2015-06-02 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:03 [zhejiang] ERROR: save_time_error:time data '2015-07-02 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:03 [zhejiang] ERROR: save_time_error:time data '2015-02-12 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:03 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-23 10:55:03 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-23 10:55:03 [zhejiang] ERROR: save_time_error:time data '2015-04-24 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:03 [zhejiang] ERROR: save_time_error:time data '2014-11-17 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:03 [zhejiang] ERROR: save_time_error:time data '2014-11-21 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:55:03 [zhejiang] ERROR: save_time_error:time data '2015-04-08 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:06 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 10:57:06 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 10:57:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 10:57:06 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 10:57:06 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 10:57:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 10:57:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 10:57:07 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 10:57:07 [scrapy.core.engine] INFO: Spider opened
2017-03-23 10:57:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 10:57:09 [zhejiang] ERROR: save_time_error:time data '2017-04-25 08' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:09 [zhejiang] ERROR: save_time_error:time data '2017-04-25 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:10 [zhejiang] ERROR: save_time_error:time data '2017-04-24 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:10 [zhejiang] ERROR: save_time_error:time data '2017-04-25 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:10 [zhejiang] ERROR: save_time_error:time data '2017-04-22 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:10 [zhejiang] ERROR: save_time_error:time data '2017-04-24 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:10 [zhejiang] ERROR: save_time_error:time data '2017-04-24 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:10 [zhejiang] ERROR: save_time_error:time data '2017-04-25 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:11 [zhejiang] ERROR: save_time_error:time data '2017-04-26 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:11 [zhejiang] ERROR: save_time_error:time data '2017-04-26 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:11 [zhejiang] ERROR: save_time_error:time data '2017-04-27 13' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:11 [zhejiang] ERROR: save_time_error:time data '2017-04-27 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:11 [zhejiang] ERROR: save_time_error:time data '2017-04-26 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:11 [zhejiang] ERROR: save_time_error:time data '2017-04-27 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:11 [zhejiang] ERROR: save_time_error:time data '2017-04-24 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:12 [zhejiang] ERROR: save_time_error:time data '2016-04-12 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:12 [zhejiang] ERROR: save_time_error:time data '2016-03-01 13' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:12 [zhejiang] ERROR: save_time_error:time data '2016-03-01 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:12 [zhejiang] ERROR: save_time_error:time data '2017-04-28 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:12 [zhejiang] ERROR: save_time_error:time data '2016-04-08 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:12 [zhejiang] ERROR: save_time_error:time data '2016-03-01 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:12 [zhejiang] ERROR: save_time_error:time data '2016-03-01 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:12 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-23 10:57:12 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-23 10:57:13 [zhejiang] ERROR: save_time_error:time data '2015-01-27 10' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:13 [zhejiang] ERROR: save_time_error:time data '2014-12-30 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:13 [zhejiang] ERROR: save_time_error:time data '2014-12-26 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:13 [zhejiang] ERROR: save_time_error:time data '2016-03-07 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:13 [zhejiang] ERROR: save_time_error:time data '2014-12-23 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:13 [zhejiang] ERROR: save_time_error:time data '2014-12-23 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:13 [zhejiang] ERROR: save_time_error:time data '2014-12-23 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:13 [zhejiang] ERROR: save_time_error:time data '2014-12-18 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:13 [zhejiang] ERROR: save_time_error:time data '2014-04-02 14' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:14 [zhejiang] ERROR: save_time_error:time data '2014-12-15 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:57:49 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 10:57:49 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 10:57:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 10:57:49 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 10:57:49 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 10:57:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 10:57:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 10:57:49 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 10:57:49 [scrapy.core.engine] INFO: Spider opened
2017-03-23 10:57:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 10:59:42 [zhejiang] ERROR: save_time_error:time data '2017-05-04 09' does not match format '%Y-%m-%d %H:%M'
2017-03-23 10:59:42 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 11:06:32 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-23 11:06:39 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 11:06:39 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 11:06:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 11:06:39 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 11:06:39 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 11:06:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 11:06:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 11:06:40 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 11:06:40 [scrapy.core.engine] INFO: Spider opened
2017-03-23 11:06:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 11:06:55 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-23 11:07:30 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 11:07:30 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 11:07:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 11:07:30 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 11:07:30 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 11:07:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 11:07:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 11:07:30 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 11:07:30 [scrapy.core.engine] INFO: Spider opened
2017-03-23 11:07:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 11:07:38 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-23 11:07:46 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 11:07:46 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 11:07:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 11:07:46 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 11:07:46 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 11:07:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 11:07:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 11:07:46 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 11:07:46 [scrapy.core.engine] INFO: Spider opened
2017-03-23 11:07:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 11:07:54 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-23 11:07:54 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-23 11:13:15 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 11:13:15 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 11:13:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 11:13:15 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 11:13:15 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 11:13:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 11:13:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 11:13:15 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 11:13:15 [scrapy.core.engine] INFO: Spider opened
2017-03-23 11:13:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 11:13:23 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-23 11:13:23 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-23 11:16:00 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 11:16:00 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 11:16:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 11:16:00 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 11:16:00 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 11:16:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 11:16:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 11:16:01 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 11:16:01 [scrapy.core.engine] INFO: Spider opened
2017-03-23 11:16:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 11:16:01 [scrapy.core.scraper] ERROR: Error downloading <GET http://ldzc.zjhrss.gov.cn/ldzc/onlinelearn/cms/bb3flist?jsoncallback=jQuery111309199566876902545_1490168372426&count=15&num=1&aab301=&type=&abb703=&abb704=&keyword=&_=1490168372427>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 11:16:01 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-23 11:16:01 [zhejiang] INFO: get_webpage_count:0
2017-03-23 11:16:01 [zhejiang] INFO: success
2017-03-23 11:16:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1734,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 23, 3, 16, 1, 696502),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 23, 3, 16, 1, 205728)}
2017-03-23 11:16:01 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-23 11:16:55 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 11:16:55 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 11:16:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 11:16:55 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 11:16:55 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 11:16:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 11:16:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 11:16:56 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 11:16:56 [scrapy.core.engine] INFO: Spider opened
2017-03-23 11:16:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 11:17:56 [scrapy.extensions.logstats] INFO: Crawled 263 pages (at 263 pages/min), scraped 3698 items (at 3698 items/min)
2017-03-23 11:18:56 [scrapy.extensions.logstats] INFO: Crawled 968 pages (at 705 pages/min), scraped 3705 items (at 7 items/min)
2017-03-23 11:19:56 [scrapy.extensions.logstats] INFO: Crawled 1418 pages (at 450 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:20:56 [scrapy.extensions.logstats] INFO: Crawled 1980 pages (at 562 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:21:56 [scrapy.extensions.logstats] INFO: Crawled 1980 pages (at 0 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:22:56 [scrapy.extensions.logstats] INFO: Crawled 1980 pages (at 0 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:23:56 [scrapy.extensions.logstats] INFO: Crawled 2164 pages (at 184 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:24:56 [scrapy.extensions.logstats] INFO: Crawled 2576 pages (at 412 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:25:56 [scrapy.extensions.logstats] INFO: Crawled 3131 pages (at 555 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:26:56 [scrapy.extensions.logstats] INFO: Crawled 3131 pages (at 0 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:27:56 [scrapy.extensions.logstats] INFO: Crawled 3131 pages (at 0 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:28:56 [scrapy.extensions.logstats] INFO: Crawled 3131 pages (at 0 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:29:56 [scrapy.extensions.logstats] INFO: Crawled 3160 pages (at 29 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:30:56 [scrapy.extensions.logstats] INFO: Crawled 3699 pages (at 539 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:31:56 [scrapy.extensions.logstats] INFO: Crawled 3699 pages (at 0 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:32:56 [scrapy.extensions.logstats] INFO: Crawled 3699 pages (at 0 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:33:56 [scrapy.extensions.logstats] INFO: Crawled 3699 pages (at 0 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:34:56 [scrapy.extensions.logstats] INFO: Crawled 3742 pages (at 43 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:35:58 [scrapy.extensions.logstats] INFO: Crawled 4370 pages (at 628 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:36:56 [scrapy.extensions.logstats] INFO: Crawled 5114 pages (at 744 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:37:56 [scrapy.extensions.logstats] INFO: Crawled 5475 pages (at 361 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:38:56 [scrapy.extensions.logstats] INFO: Crawled 6160 pages (at 685 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:39:56 [scrapy.extensions.logstats] INFO: Crawled 6827 pages (at 667 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:40:56 [scrapy.extensions.logstats] INFO: Crawled 7144 pages (at 317 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:41:56 [scrapy.extensions.logstats] INFO: Crawled 7144 pages (at 0 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:42:56 [scrapy.extensions.logstats] INFO: Crawled 7144 pages (at 0 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:43:56 [scrapy.extensions.logstats] INFO: Crawled 7251 pages (at 107 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:44:56 [scrapy.extensions.logstats] INFO: Crawled 7830 pages (at 579 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:45:56 [scrapy.extensions.logstats] INFO: Crawled 8656 pages (at 826 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:46:59 [scrapy.extensions.logstats] INFO: Crawled 9250 pages (at 594 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:47:56 [scrapy.extensions.logstats] INFO: Crawled 9333 pages (at 83 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:48:56 [scrapy.extensions.logstats] INFO: Crawled 9898 pages (at 565 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:49:56 [scrapy.extensions.logstats] INFO: Crawled 10662 pages (at 764 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:50:56 [scrapy.extensions.logstats] INFO: Crawled 11237 pages (at 575 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:51:56 [scrapy.extensions.logstats] INFO: Crawled 11237 pages (at 0 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:52:56 [scrapy.extensions.logstats] INFO: Crawled 11237 pages (at 0 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:53:56 [scrapy.extensions.logstats] INFO: Crawled 11273 pages (at 36 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:54:56 [scrapy.extensions.logstats] INFO: Crawled 11437 pages (at 164 pages/min), scraped 3705 items (at 0 items/min)
2017-03-23 11:55:56 [scrapy.extensions.logstats] INFO: Crawled 12036 pages (at 599 pages/min), scraped 5502 items (at 1797 items/min)
2017-03-23 11:56:56 [scrapy.extensions.logstats] INFO: Crawled 12325 pages (at 289 pages/min), scraped 9866 items (at 4364 items/min)
2017-03-23 11:57:56 [scrapy.extensions.logstats] INFO: Crawled 12600 pages (at 275 pages/min), scraped 14089 items (at 4223 items/min)
2017-03-23 11:58:56 [scrapy.extensions.logstats] INFO: Crawled 12881 pages (at 281 pages/min), scraped 18208 items (at 4119 items/min)
2017-03-23 11:59:56 [scrapy.extensions.logstats] INFO: Crawled 13169 pages (at 288 pages/min), scraped 22491 items (at 4283 items/min)
2017-03-23 12:00:56 [scrapy.extensions.logstats] INFO: Crawled 13367 pages (at 198 pages/min), scraped 25600 items (at 3109 items/min)
2017-03-23 12:01:56 [scrapy.extensions.logstats] INFO: Crawled 13367 pages (at 0 pages/min), scraped 25600 items (at 0 items/min)
2017-03-23 12:02:56 [scrapy.extensions.logstats] INFO: Crawled 13367 pages (at 0 pages/min), scraped 25600 items (at 0 items/min)
2017-03-23 12:03:56 [scrapy.extensions.logstats] INFO: Crawled 13367 pages (at 0 pages/min), scraped 25600 items (at 0 items/min)
2017-03-23 12:04:56 [scrapy.extensions.logstats] INFO: Crawled 13588 pages (at 221 pages/min), scraped 28777 items (at 3177 items/min)
2017-03-23 12:05:56 [scrapy.extensions.logstats] INFO: Crawled 13873 pages (at 285 pages/min), scraped 33054 items (at 4277 items/min)
2017-03-23 12:06:56 [scrapy.extensions.logstats] INFO: Crawled 14166 pages (at 293 pages/min), scraped 37456 items (at 4402 items/min)
2017-03-23 12:07:56 [scrapy.extensions.logstats] INFO: Crawled 14417 pages (at 251 pages/min), scraped 41262 items (at 3806 items/min)
2017-03-23 12:08:56 [scrapy.extensions.logstats] INFO: Crawled 14676 pages (at 259 pages/min), scraped 45060 items (at 3798 items/min)
2017-03-23 12:09:56 [scrapy.extensions.logstats] INFO: Crawled 14929 pages (at 253 pages/min), scraped 48854 items (at 3794 items/min)
2017-03-23 12:10:56 [scrapy.extensions.logstats] INFO: Crawled 15105 pages (at 176 pages/min), scraped 51670 items (at 2816 items/min)
2017-03-23 12:11:56 [scrapy.extensions.logstats] INFO: Crawled 15105 pages (at 0 pages/min), scraped 51670 items (at 0 items/min)
2017-03-23 12:12:56 [scrapy.extensions.logstats] INFO: Crawled 15105 pages (at 0 pages/min), scraped 51670 items (at 0 items/min)
2017-03-23 12:13:56 [scrapy.extensions.logstats] INFO: Crawled 15155 pages (at 50 pages/min), scraped 52398 items (at 728 items/min)
2017-03-23 12:14:56 [scrapy.extensions.logstats] INFO: Crawled 15418 pages (at 263 pages/min), scraped 56241 items (at 3843 items/min)
2017-03-23 12:15:56 [scrapy.extensions.logstats] INFO: Crawled 15671 pages (at 253 pages/min), scraped 60010 items (at 3769 items/min)
2017-03-23 12:16:56 [scrapy.extensions.logstats] INFO: Crawled 15941 pages (at 270 pages/min), scraped 64032 items (at 4022 items/min)
2017-03-23 12:17:56 [scrapy.extensions.logstats] INFO: Crawled 16202 pages (at 261 pages/min), scraped 67982 items (at 3950 items/min)
2017-03-23 12:18:56 [scrapy.extensions.logstats] INFO: Crawled 16471 pages (at 269 pages/min), scraped 72019 items (at 4037 items/min)
2017-03-23 12:19:56 [scrapy.extensions.logstats] INFO: Crawled 16726 pages (at 255 pages/min), scraped 75884 items (at 3865 items/min)
2017-03-23 12:20:56 [scrapy.extensions.logstats] INFO: Crawled 16922 pages (at 196 pages/min), scraped 78925 items (at 3041 items/min)
2017-03-23 12:21:56 [scrapy.extensions.logstats] INFO: Crawled 16922 pages (at 0 pages/min), scraped 78925 items (at 0 items/min)
2017-03-23 12:22:56 [scrapy.extensions.logstats] INFO: Crawled 16922 pages (at 0 pages/min), scraped 78925 items (at 0 items/min)
2017-03-23 12:23:56 [scrapy.extensions.logstats] INFO: Crawled 16951 pages (at 29 pages/min), scraped 79263 items (at 338 items/min)
2017-03-23 12:24:57 [scrapy.extensions.logstats] INFO: Crawled 17022 pages (at 71 pages/min), scraped 80237 items (at 974 items/min)
2017-03-23 12:25:58 [scrapy.extensions.logstats] INFO: Crawled 17097 pages (at 75 pages/min), scraped 81228 items (at 991 items/min)
2017-03-23 12:26:56 [scrapy.extensions.logstats] INFO: Crawled 17302 pages (at 205 pages/min), scraped 84489 items (at 3261 items/min)
2017-03-23 12:27:56 [scrapy.extensions.logstats] INFO: Crawled 17525 pages (at 223 pages/min), scraped 87890 items (at 3401 items/min)
2017-03-23 12:28:56 [scrapy.extensions.logstats] INFO: Crawled 17747 pages (at 222 pages/min), scraped 91148 items (at 3258 items/min)
2017-03-23 12:29:56 [scrapy.extensions.logstats] INFO: Crawled 17992 pages (at 245 pages/min), scraped 94869 items (at 3721 items/min)
2017-03-23 12:30:56 [scrapy.extensions.logstats] INFO: Crawled 18176 pages (at 184 pages/min), scraped 97735 items (at 2866 items/min)
2017-03-23 12:31:56 [scrapy.extensions.logstats] INFO: Crawled 18176 pages (at 0 pages/min), scraped 97735 items (at 0 items/min)
2017-03-23 12:32:56 [scrapy.extensions.logstats] INFO: Crawled 18176 pages (at 0 pages/min), scraped 97735 items (at 0 items/min)
2017-03-23 12:33:56 [scrapy.extensions.logstats] INFO: Crawled 18195 pages (at 19 pages/min), scraped 97735 items (at 0 items/min)
2017-03-23 12:34:56 [scrapy.extensions.logstats] INFO: Crawled 18716 pages (at 521 pages/min), scraped 97735 items (at 0 items/min)
2017-03-23 12:35:56 [scrapy.extensions.logstats] INFO: Crawled 19304 pages (at 588 pages/min), scraped 97765 items (at 30 items/min)
2017-03-23 12:36:56 [scrapy.extensions.logstats] INFO: Crawled 19603 pages (at 299 pages/min), scraped 101638 items (at 3873 items/min)
2017-03-23 12:37:59 [scrapy.extensions.logstats] INFO: Crawled 19818 pages (at 215 pages/min), scraped 104908 items (at 3270 items/min)
2017-03-23 12:38:46 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-23 12:38:46 [zhejiang] INFO: get_webpage_count:107770
2017-03-23 12:38:46 [zhejiang] INFO: success
2017-03-23 12:38:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3521,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3401,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 35,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 68,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 17,
 'downloader/request_bytes': 20417989,
 'downloader/request_count': 23521,
 'downloader/request_method_count/GET': 23521,
 'downloader/response_bytes': 44263179,
 'downloader/response_count': 20000,
 'downloader/response_status_count/200': 20000,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 23, 4, 38, 46, 382321),
 'item_scraped_count': 107770,
 'log_count/INFO': 90,
 'log_count/WARNING': 2,
 'request_depth_max': 1,
 'response_received_count': 20000,
 'scheduler/dequeued': 23521,
 'scheduler/dequeued/memory': 23521,
 'scheduler/enqueued': 23521,
 'scheduler/enqueued/memory': 23521,
 'start_time': datetime.datetime(2017, 3, 23, 3, 16, 56, 17241)}
2017-03-23 12:38:46 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-23 14:30:31 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 14:30:31 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 14:30:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 14:30:31 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:30:31 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:30:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 14:30:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 14:30:31 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 14:30:31 [scrapy.core.engine] INFO: Spider opened
2017-03-23 14:30:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 14:30:32 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:33 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:34 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:35 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:30:53 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54857.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:53 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54931.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:53 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54767.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201605/57572.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54774.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201508/54349.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201508/54170.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54770.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201508/54171.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201508/54172.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54644.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201508/54348.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54652.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201508/54168.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54647.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201510/55136.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54933.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54772.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54651.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54769.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54934.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201510/55133.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201508/53990.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201510/55011.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201510/55195.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201510/55197.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201510/55009.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201510/55008.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201510/55130.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201510/55006.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201510/55129.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54649.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:54 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201510/55132.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55304.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201510/55196.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201507/53212.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55303.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55305.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201504/50749.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201510/55131.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55371.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55269.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55372.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55367.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55452.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201412/49646.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55368.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55529.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55450.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55449.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55370.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55451.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55530.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55366.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201507/52910.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/55636.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/55637.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/55719.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201412/49720.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/55635.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201602/56720.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/55721.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/56076.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/55720.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/56075.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/55638.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/55718.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/55722.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/56074.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/56079.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/56073.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:55 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/56032.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/56031.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/56078.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201512/55634.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201601/56239.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201506/51397.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201601/56131.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201601/56245.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201601/56241.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201601/56244.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201601/56347.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201601/56349.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201601/56132.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201601/56133.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201601/56348.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201601/56240.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201601/56346.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201601/56238.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201601/56243.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201507/51763.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201509/54932.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201602/56571.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201602/56573.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201602/56712.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201602/56718.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201602/56656.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201602/56653.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201602/56716.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201602/56715.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201602/56719.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201602/56654.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56773.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56936.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56774.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201602/56714.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56886.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56775.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201507/51525.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56885.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56887.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56888.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56889.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/57069.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56933.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56934.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/57070.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56986.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56932.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201604/57370.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56776.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201604/57369.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201605/57708.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201605/57568.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201605/57570.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201511/55369.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201605/57709.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201604/57181.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201603/56935.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201605/57571.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201606/57937.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201607/58136.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201507/53211.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201506/51439.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201606/57936.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201606/57933.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201606/57934.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201606/57935.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201607/58138.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201607/58298.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201607/58139.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201607/58300.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201607/58137.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201611/59476.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201611/59477.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201611/59473.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201611/59479.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201611/59478.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201611/59482.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201611/59480.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201703/60346.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201611/59481.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201703/60345.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201703/60349.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201612/59740.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201611/59483.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201703/60347.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201703/60348.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201701/60067.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201701/60066.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201612/59980.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201612/59978.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201612/59979.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201701/60064.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201701/60065.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201612/59742.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201612/59984.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201611/59484.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/201612/59741.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:30:58 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-23 14:30:58 [guangdong] INFO: get_webpage_count:0
2017-03-23 14:30:58 [guangdong] INFO: success
2017-03-23 14:30:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 507,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 502,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 5,
 'downloader/request_bytes': 384036,
 'downloader/request_count': 640,
 'downloader/request_method_count/GET': 640,
 'downloader/response_bytes': 2964738,
 'downloader/response_count': 133,
 'downloader/response_status_count/200': 133,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 23, 6, 30, 58, 942011),
 'log_count/ERROR': 301,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'request_depth_max': 1,
 'response_received_count': 133,
 'scheduler/dequeued': 640,
 'scheduler/dequeued/memory': 640,
 'scheduler/enqueued': 640,
 'scheduler/enqueued/memory': 640,
 'start_time': datetime.datetime(2017, 3, 23, 6, 30, 31, 569584)}
2017-03-23 14:30:58 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-23 14:31:48 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 14:31:48 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 14:31:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 14:31:48 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:31:48 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:31:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 14:31:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 14:31:48 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 14:31:48 [scrapy.core.engine] INFO: Spider opened
2017-03-23 14:31:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 14:31:49 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:49 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:49 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:49 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:49 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:49 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:50 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:51 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:51 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:51 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:51 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:51 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:51 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:51 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:51 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:51 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:51 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:52 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:52 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:52 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:52 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:52 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:52 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:52 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:52 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:52 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:53 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:54 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:54 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:54 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:54 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:54 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:54 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:54 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:54 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:54 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:54 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:54 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:55 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:55 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:55 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:55 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:55 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:55 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:55 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:55 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:56 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:57 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:57 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:57 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:57 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:57 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:57 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:57 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:57 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:57 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:57 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:58 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:58 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:58 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:58 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:58 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:58 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:59 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:59 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:59 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:59 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:59 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:59 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:59 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:31:59 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:01 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:00 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:01 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:01 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:01 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:01 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:01 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:01 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:01 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:01 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:02 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:02 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:02 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:02 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:02 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:02 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:03 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:03 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:03 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:05 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:05 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:05 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:06 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:07 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:07 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:07 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:07 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:07 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:07 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:07 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:07 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:07 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:07 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:07 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:07 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:07 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:08 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:08 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:08 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:08 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:08 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:08 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:08 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:08 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:08 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:08 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:08 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:08 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:08 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:08 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:09 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:10 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:11 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:11 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:11 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:11 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:11 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:11 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:11 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:11 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:11 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:11 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:11 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:11 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:11 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:12 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:12 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:12 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:12 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:12 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:12 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:12 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:12 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:12 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:12 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:12 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:12 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:12 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:13 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:13 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:13 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:13 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:13 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:13 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:13 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:13 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:13 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:13 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:13 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:13 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:13 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:14 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:14 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:14 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:14 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:14 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:14 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:14 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:14 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:14 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:14 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:14 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:14 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:14 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:15 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:15 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:15 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:15 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:15 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:32:15 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-23 14:32:15 [guangdong] INFO: get_webpage_count:0
2017-03-23 14:32:15 [guangdong] INFO: success
2017-03-23 14:32:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 181314,
 'downloader/request_count': 302,
 'downloader/request_method_count/GET': 302,
 'downloader/response_bytes': 6512396,
 'downloader/response_count': 302,
 'downloader/response_status_count/200': 302,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 23, 6, 32, 15, 871040),
 'log_count/ERROR': 301,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'request_depth_max': 1,
 'response_received_count': 302,
 'scheduler/dequeued': 302,
 'scheduler/dequeued/memory': 302,
 'scheduler/enqueued': 302,
 'scheduler/enqueued/memory': 302,
 'start_time': datetime.datetime(2017, 3, 23, 6, 31, 48, 601686)}
2017-03-23 14:32:15 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-23 14:33:34 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 14:33:34 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 14:33:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 14:33:35 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:33:35 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:33:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 14:33:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 14:33:35 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 14:33:35 [scrapy.core.engine] INFO: Spider opened
2017-03-23 14:33:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 14:33:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:36 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:37 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:38 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:39 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:40 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:41 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:42 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:43 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:44 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:45 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:46 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:47 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:48 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:49 [guangdong] ERROR: parse_webpage_error:'list' object has no attribute 'find'
2017-03-23 14:33:49 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-23 14:33:49 [guangdong] INFO: get_webpage_count:0
2017-03-23 14:33:49 [guangdong] INFO: success
2017-03-23 14:33:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 181354,
 'downloader/request_count': 302,
 'downloader/request_method_count/GET': 302,
 'downloader/response_bytes': 6512227,
 'downloader/response_count': 302,
 'downloader/response_status_count/200': 302,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 23, 6, 33, 49, 6807),
 'log_count/ERROR': 301,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'request_depth_max': 1,
 'response_received_count': 302,
 'scheduler/dequeued': 302,
 'scheduler/dequeued/memory': 302,
 'scheduler/enqueued': 302,
 'scheduler/enqueued/memory': 302,
 'start_time': datetime.datetime(2017, 3, 23, 6, 33, 35, 372406)}
2017-03-23 14:33:49 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-23 14:35:42 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 14:35:42 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 14:35:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 14:35:42 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:35:42 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:35:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 14:35:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 14:35:42 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 14:35:42 [scrapy.core.engine] INFO: Spider opened
2017-03-23 14:35:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: save_time_error:time data '2016年6月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: save_time_error:time data '2015年1月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: save_time_error:time data '2015年1月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: save_time_error:time data '2015年1月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: save_time_error:time data '2015年1月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: save_time_error:time data '2015年1月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:44 [guangdong] ERROR: save_time_error:time data '2015年1月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:44 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年1月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年1月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年2月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年2月4 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年2月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年2月6 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年1月3 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年2月3 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年2月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年2月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年2月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年2月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年2月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年3月4 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年2月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年3月4 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年3月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:45 [guangdong] ERROR: save_time_error:time data '2015年3月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: save_time_error:time data '2015年3月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: save_time_error:time data '2015年3月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: save_time_error:time data '2015年3月3 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: save_time_error:time data '2015年3月3 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: save_time_error:time data '2015年4月7 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: save_time_error:time data '2015年4月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: save_time_error:time data '2015年4月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: save_time_error:time data '2015年4月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: save_time_error:time data '2015年4月3 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: save_time_error:time data '2015年4月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: save_time_error:time data '2015年5月5 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: save_time_error:time data '2015年5月6 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: save_time_error:time data '2015年5月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: save_time_error:time data '2015年5月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: save_time_error:time data '2015年6月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: save_time_error:time data '2015年6月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: save_time_error:time data '2015年6月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: save_time_error:time data '2015年6月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: save_time_error:time data '2015年7月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: save_time_error:time data '2015年7月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: save_time_error:time data '2015年7月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: save_time_error:time data '2015年7月3 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: save_time_error:time data '2015年7月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: save_time_error:time data '2015年7月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: save_time_error:time data '2015年7月3 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: save_time_error:time data '2015年7月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: save_time_error:time data '2015年7月3 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: save_time_error:time data '2014年5月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: save_time_error:time data '2015年8月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: save_time_error:time data '2015年8月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: save_time_error:time data '2015年8月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: save_time_error:time data '2015年8月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: save_time_error:time data '2015年8月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:49 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: save_time_error:time data '2015年9月9 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: save_time_error:time data '2015年9月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: save_time_error:time data '2015年9月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: save_time_error:time data '2015年9月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: save_time_error:time data '2015年9月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: save_time_error:time data '2015年9月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: save_time_error:time data '2015年9月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: save_time_error:time data '2015年10月 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: save_time_error:time data '2015年10月 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:50 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: save_time_error:time data '2015年10月 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: save_time_error:time data '2015年10月 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: save_time_error:time data '2015年11月 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: save_time_error:time data '2015年4月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: save_time_error:time data '2015年11月 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: save_time_error:time data '2015年11月 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:52 [guangdong] ERROR: save_time_error:time data '2015年11月 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:52 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: save_time_error:time data '2015年11月 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: save_time_error:time data '2015年11月 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: save_time_error:time data '2015年12月 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: save_time_error:time data '2015年11月 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: save_time_error:time data '2015年12月 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: save_time_error:time data '2015年12月 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: save_time_error:time data '2015年12月 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: save_time_error:time data '2016年2月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:54 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: save_time_error:time data '2016年2月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: save_time_error:time data '2016年2月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: save_time_error:time data '2016年3月3 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: save_time_error:time data '2016年3月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: save_time_error:time data '2016年3月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: save_time_error:time data '2016年3月8 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: save_time_error:time data '2015年11月 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: save_time_error:time data '2015年6月3 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:35:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:35:58 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-23 14:35:58 [guangdong] INFO: get_webpage_count:0
2017-03-23 14:35:58 [guangdong] INFO: success
2017-03-23 14:35:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 181280,
 'downloader/request_count': 302,
 'downloader/request_method_count/GET': 302,
 'downloader/response_bytes': 6512227,
 'downloader/response_count': 302,
 'downloader/response_status_count/200': 302,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 23, 6, 35, 58, 572295),
 'log_count/ERROR': 391,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'request_depth_max': 1,
 'response_received_count': 302,
 'scheduler/dequeued': 302,
 'scheduler/dequeued/memory': 302,
 'scheduler/enqueued': 302,
 'scheduler/enqueued/memory': 302,
 'start_time': datetime.datetime(2017, 3, 23, 6, 35, 42, 700274)}
2017-03-23 14:35:58 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-23 14:36:50 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 14:36:50 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 14:36:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 14:36:50 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:36:50 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:36:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 14:36:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 14:36:50 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 14:36:50 [scrapy.core.engine] INFO: Spider opened
2017-03-23 14:36:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 14:36:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:56 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:56 [guangdong] ERROR: save_time_error:time data '2015年1月12 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:36:56 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:36:57 [guangdong] ERROR: save_time_error:time data '2015年1月14 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:36:57 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:36:57 [guangdong] ERROR: save_time_error:time data '2015年1月12 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:36:57 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:36:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:57 [guangdong] ERROR: save_time_error:time data '2015年1月20 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:36:57 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:36:57 [guangdong] ERROR: save_time_error:time data '2015年1月16 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:36:57 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:36:57 [guangdong] ERROR: save_time_error:time data '2015年1月23 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:36:57 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:36:58 [guangdong] ERROR: save_time_error:time data '2015年1月19 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:36:58 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:36:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:58 [guangdong] ERROR: save_time_error:time data '2015年1月30 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:36:58 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:36:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:59 [guangdong] ERROR: save_time_error:time data '2015年1月26 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:36:59 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:36:59 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:36:59 [guangdong] ERROR: save_time_error:time data '2015年2月10 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:36:59 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:36:59 [guangdong] ERROR: save_time_error:time data '2015年2月11 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:36:59 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:36:59 [guangdong] ERROR: save_time_error:time data '2015年2月12 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:36:59 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:36:59 [guangdong] ERROR: save_time_error:time data '2015年2月11 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:36:59 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:36:59 [guangdong] ERROR: save_time_error:time data '2015年2月12 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:36:59 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:00 [guangdong] ERROR: save_time_error:time data '2015年3月13 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:00 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:00 [guangdong] ERROR: save_time_error:time data '2015年2月13 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:00 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:00 [guangdong] ERROR: save_time_error:time data '2015年3月18 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:00 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:00 [guangdong] ERROR: save_time_error:time data '2015年3月20 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:00 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:01 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:01 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:01 [guangdong] ERROR: save_time_error:time data '2015年3月24 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:01 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:01 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:01 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:01 [guangdong] ERROR: save_time_error:time data '2015年3月31 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:01 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:01 [guangdong] ERROR: save_time_error:time data '2015年3月30 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:01 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:01 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:01 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:01 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:01 [guangdong] ERROR: save_time_error:time data '2015年4月15 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:01 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:01 [guangdong] ERROR: save_time_error:time data '2015年4月17 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:01 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:02 [guangdong] ERROR: save_time_error:time data '2015年4月14 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:02 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:02 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:02 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:02 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:02 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:02 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:02 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:02 [guangdong] ERROR: save_time_error:time data '2015年4月28 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:02 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:02 [guangdong] ERROR: save_time_error:time data '2015年4月30 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:02 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:02 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:02 [guangdong] ERROR: save_time_error:time data '2015年5月12 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:02 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:02 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:03 [guangdong] ERROR: save_time_error:time data '2015年5月13 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:03 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:03 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:03 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:03 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:03 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:03 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:03 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:03 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:03 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:03 [guangdong] ERROR: save_time_error:time data '2015年6月10 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:03 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:04 [guangdong] ERROR: save_time_error:time data '2015年6月11 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:04 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:04 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:04 [guangdong] ERROR: save_time_error:time data '2015年6月12 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:04 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:04 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:04 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:04 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:04 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:04 [guangdong] ERROR: save_time_error:time data '2015年6月17 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:04 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:04 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:05 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:05 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:05 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:05 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:05 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:05 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:05 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:05 [guangdong] ERROR: save_time_error:time data '2015年7月10 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:05 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:05 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:06 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:07 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:07 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:07 [guangdong] ERROR: save_time_error:time data '2015年7月22 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:07 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:07 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:07 [guangdong] ERROR: save_time_error:time data '2015年7月30 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:07 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:07 [guangdong] ERROR: save_time_error:time data '2015年7月23 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:07 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:07 [guangdong] ERROR: save_time_error:time data '2015年7月21 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:07 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:07 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:07 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:08 [guangdong] ERROR: save_time_error:time data '2014年5月15 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:08 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:08 [guangdong] ERROR: save_time_error:time data '2015年7月31 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:08 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:08 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:08 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:08 [guangdong] ERROR: save_time_error:time data '2015年8月18 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:08 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:08 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:08 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:08 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:08 [guangdong] ERROR: save_time_error:time data '2015年8月20 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:08 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:08 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:08 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:08 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:09 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:09 [guangdong] ERROR: save_time_error:time data '2015年8月28 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:09 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:09 [guangdong] ERROR: save_time_error:time data '2015年8月26 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:09 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:09 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:09 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:09 [guangdong] ERROR: save_time_error:time data '2015年8月27 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:09 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:09 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:10 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:10 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:10 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:10 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:10 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:10 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:10 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:10 [guangdong] ERROR: save_time_error:time data '2015年9月17 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:10 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:10 [guangdong] ERROR: save_time_error:time data '2015年9月14 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:10 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:10 [guangdong] ERROR: save_time_error:time data '2015年9月15 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:10 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:11 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:11 [guangdong] ERROR: save_time_error:time data '2015年9月22 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:11 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:11 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:11 [guangdong] ERROR: save_time_error:time data '2015年9月23 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:11 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:11 [guangdong] ERROR: save_time_error:time data '2015年9月29 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:11 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:11 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:11 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:11 [guangdong] ERROR: save_time_error:time data '2015年10月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:11 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:12 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:12 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:12 [guangdong] ERROR: save_time_error:time data '2015年10月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:12 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:12 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:12 [guangdong] ERROR: save_time_error:time data '2015年10月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:12 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:12 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:12 [guangdong] ERROR: save_time_error:time data '2015年10月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:12 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:13 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:13 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:13 [guangdong] ERROR: save_time_error:time data '2015年4月29 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:13 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:13 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:13 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:13 [guangdong] ERROR: save_time_error:time data '2015年11月6 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:13 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:13 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:13 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:13 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:13 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:14 [guangdong] ERROR: save_time_error:time data '2015年11月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:14 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:14 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:14 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:14 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:14 [guangdong] ERROR: save_time_error:time data '2015年11月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:14 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:14 [guangdong] ERROR: save_time_error:time data '2015年11月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:14 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:14 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:14 [guangdong] ERROR: save_time_error:time data '2015年11月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:14 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:15 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:15 [guangdong] ERROR: save_time_error:time data '2015年12月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:15 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:15 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:15 [guangdong] ERROR: save_time_error:time data '2015年11月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:15 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:15 [guangdong] ERROR: save_time_error:time data '2015年11月3 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:15 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:15 [guangdong] ERROR: save_time_error:time data '2015年12月3 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:15 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:15 [guangdong] ERROR: save_time_error:time data '2015年12月9 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:15 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:16 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:16 [guangdong] ERROR: save_time_error:time data '2015年12月7 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:16 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:16 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:16 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:16 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:16 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:16 [guangdong] ERROR: save_time_error:time data '2016年2月29 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:16 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:16 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:16 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:16 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:16 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:18 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:18 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:18 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:18 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:18 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:18 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:18 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:18 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:18 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:18 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:18 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:18 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:18 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:19 [guangdong] ERROR: save_time_error:time data '2016年2月17 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:19 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:19 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:19 [guangdong] ERROR: save_time_error:time data '2016年2月23 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:19 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:19 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:19 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:19 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:19 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:19 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:19 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:20 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:20 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:20 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:20 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:20 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:20 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:20 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:20 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:23 [guangdong] ERROR: save_time_error:time data '2015年11月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:23 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:23 [guangdong] ERROR: save_time_error:time data '2015年6月30 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:23 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: save_time_error:time data '2016年6月27 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:25 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:25 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:25 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:25 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:25 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-23 14:37:25 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:25 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-23 14:37:25 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:37:25 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:40:47 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 14:40:47 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 14:40:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 14:40:47 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:40:47 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:40:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 14:40:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 14:40:47 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 14:40:47 [scrapy.core.engine] INFO: Spider opened
2017-03-23 14:40:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 14:40:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/list.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:40:48 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-23 14:40:48 [guangdong] INFO: get_webpage_count:0
2017-03-23 14:40:48 [guangdong] INFO: success
2017-03-23 14:40:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1434,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 23, 6, 40, 48, 71759),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 23, 6, 40, 47, 522316)}
2017-03-23 14:40:48 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-23 14:40:52 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 14:40:52 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 14:40:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 14:40:52 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:40:52 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:40:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 14:40:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 14:40:52 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 14:40:52 [scrapy.core.engine] INFO: Spider opened
2017-03-23 14:40:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 14:40:52 [scrapy.core.scraper] ERROR: Error downloading <GET http://www.gdhrss.gov.cn/publicfiles/business/htmlfiles/tjzcw/ktgg/list.html>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-23 14:40:52 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-23 14:40:52 [guangdong] INFO: get_webpage_count:0
2017-03-23 14:40:52 [guangdong] INFO: success
2017-03-23 14:40:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1434,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 23, 6, 40, 52, 868644),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 23, 6, 40, 52, 427257)}
2017-03-23 14:40:52 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-23 14:41:17 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 14:41:17 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 14:41:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 14:41:17 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:41:17 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:41:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 14:41:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 14:41:17 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 14:41:17 [scrapy.core.engine] INFO: Spider opened
2017-03-23 14:41:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 14:41:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:25 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:25 [guangdong] ERROR: save_time_error:time data '2015年1月12 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:25 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:25 [guangdong] ERROR: save_time_error:time data '2015年1月14 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:25 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:25 [guangdong] ERROR: save_time_error:time data '2015年1月16 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:25 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:25 [guangdong] ERROR: save_time_error:time data '2015年1月19 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:25 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:25 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:26 [guangdong] ERROR: save_time_error:time data '2015年1月20 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:26 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:27 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:27 [guangdong] ERROR: save_time_error:time data '2015年1月26 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:27 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:27 [guangdong] ERROR: save_time_error:time data '2015年1月30 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:27 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:27 [guangdong] ERROR: save_time_error:time data '2015年1月23 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:27 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:27 [guangdong] ERROR: save_time_error:time data '2015年1月12 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:27 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:28 [guangdong] ERROR: save_time_error:time data '2015年2月11 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:28 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:28 [guangdong] ERROR: save_time_error:time data '2015年2月13 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:28 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:28 [guangdong] ERROR: save_time_error:time data '2015年2月12 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:28 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:29 [guangdong] ERROR: save_time_error:time data '2015年2月11 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:29 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:29 [guangdong] ERROR: save_time_error:time data '2015年2月12 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:29 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:29 [guangdong] ERROR: save_time_error:time data '2015年2月10 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:29 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:29 [guangdong] ERROR: save_time_error:time data '2015年3月13 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:29 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:30 [guangdong] ERROR: save_time_error:time data '2015年3月18 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:30 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:30 [guangdong] ERROR: save_time_error:time data '2015年3月24 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:30 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:30 [guangdong] ERROR: save_time_error:time data '2015年3月20 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:30 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:31 [guangdong] ERROR: save_time_error:time data '2015年3月31 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:31 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:32 [guangdong] ERROR: save_time_error:time data '2015年4月15 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:32 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:32 [guangdong] ERROR: save_time_error:time data '2015年4月14 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:32 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:32 [guangdong] ERROR: save_time_error:time data '2015年3月30 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:32 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:32 [guangdong] ERROR: save_time_error:time data '2015年4月17 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:32 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:33 [guangdong] ERROR: save_time_error:time data '2015年4月28 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:33 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:33 [guangdong] ERROR: save_time_error:time data '2015年5月12 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:33 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:34 [guangdong] ERROR: save_time_error:time data '2015年5月13 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:34 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:34 [guangdong] ERROR: save_time_error:time data '2015年4月30 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:34 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:34 [guangdong] ERROR: save_time_error:time data '2015年6月12 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:34 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:35 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:35 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:35 [guangdong] ERROR: save_time_error:time data '2015年6月11 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:35 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:35 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:35 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:35 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:35 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:35 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:35 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:35 [guangdong] ERROR: save_time_error:time data '2015年6月10 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:35 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:35 [guangdong] ERROR: save_time_error:time data '2015年6月17 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:35 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:35 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:35 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:35 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: save_time_error:time data '2015年7月10 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: save_time_error:time data '2015年7月23 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: save_time_error:time data '2015年7月30 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: save_time_error:time data '2015年7月22 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: save_time_error:time data '2015年7月21 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:36 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: save_time_error:time data '2014年5月15 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: save_time_error:time data '2015年7月31 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: save_time_error:time data '2015年8月18 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: save_time_error:time data '2015年8月20 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: save_time_error:time data '2015年8月27 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: save_time_error:time data '2015年8月26 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: save_time_error:time data '2015年8月28 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:37 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: save_time_error:time data '2015年9月14 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: save_time_error:time data '2015年9月15 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: save_time_error:time data '2015年9月23 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: save_time_error:time data '2015年9月17 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: save_time_error:time data '2015年9月22 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: save_time_error:time data '2015年9月29 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: save_time_error:time data '2015年10月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: save_time_error:time data '2015年10月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: save_time_error:time data '2015年10月1 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: save_time_error:time data '2015年10月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:38 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:39 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:39 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:39 [guangdong] ERROR: save_time_error:time data '2015年4月29 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:39 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:39 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:39 [guangdong] ERROR: save_time_error:time data '2015年11月6 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:39 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:39 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:39 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:39 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:39 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:40 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:40 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:40 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:40 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:40 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:40 [guangdong] ERROR: save_time_error:time data '2015年11月1 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:40 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:40 [guangdong] ERROR: save_time_error:time data '2015年11月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:40 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:40 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: save_time_error:time data '2015年11月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: save_time_error:time data '2015年11月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: save_time_error:time data '2015年12月3 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: save_time_error:time data '2015年11月3 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: save_time_error:time data '2015年11月2 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: save_time_error:time data '2015年12月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: save_time_error:time data '2015年12月9 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: save_time_error:time data '2015年12月7 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: save_time_error:time data '2016年2月29 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:41 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:42 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:42 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:42 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:42 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:42 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:42 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:42 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:42 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:42 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:43 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:43 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:43 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:43 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:43 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:43 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:43 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:43 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:43 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:43 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:43 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:44 [guangdong] ERROR: save_time_error:time data '2016年2月17 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:44 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:44 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:45 [guangdong] ERROR: save_time_error:time data '2016年2月23 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:45 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:45 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:45 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:45 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:46 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:47 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:51 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:52 [guangdong] ERROR: save_time_error:time data '2015年11月2 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:52 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:53 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:55 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:57 [guangdong] ERROR: save_time_error:time data '2015年6月30 08:45' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:57 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:58 [guangdong] ERROR: save_time_error:time data '2016年6月27 14:15' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:41:58 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:41:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:59 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:59 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:59 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:59 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:59 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:59 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:59 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:59 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:59 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:59 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:41:59 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:01 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:01 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:01 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:02 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:02 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:03 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:03 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:05 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:05 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:05 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:08 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:42:08 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-23 14:42:08 [guangdong] INFO: get_webpage_count:18
2017-03-23 14:42:08 [guangdong] INFO: success
2017-03-23 14:42:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 181365,
 'downloader/request_count': 302,
 'downloader/request_method_count/GET': 302,
 'downloader/response_bytes': 6512229,
 'downloader/response_count': 302,
 'downloader/response_status_count/200': 302,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 23, 6, 42, 8, 867168),
 'item_scraped_count': 18,
 'log_count/ERROR': 355,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'request_depth_max': 1,
 'response_received_count': 302,
 'scheduler/dequeued': 302,
 'scheduler/dequeued/memory': 302,
 'scheduler/enqueued': 302,
 'scheduler/enqueued/memory': 302,
 'start_time': datetime.datetime(2017, 3, 23, 6, 41, 17, 567368)}
2017-03-23 14:42:08 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-23 14:45:38 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 14:45:38 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 14:45:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 14:45:38 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:45:38 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:45:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 14:45:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 14:45:38 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 14:45:38 [scrapy.core.engine] INFO: Spider opened
2017-03-23 14:45:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 14:45:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:48 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:52 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:57 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:58 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:59 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:45:59 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:00 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:03 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:03 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:03 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:03 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:04 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:04 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:04 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:04 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:05 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:05 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:05 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:06 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:06 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:06 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:07 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:07 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:07 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:09 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:12 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:12 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:13 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:13 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:13 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:13 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:14 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:14 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:14 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:16 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:17 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:19 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:19 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:19 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:20 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:20 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:22 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:23 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:24 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:25 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:25 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:26 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:26 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:26 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:26 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:26 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:26 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:26 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:27 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:27 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:27 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:27 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:27 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:27 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:28 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:29 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:30 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:31 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:32 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:33 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [guangdong] ERROR: parse_webpage_error:local variable 'hours_get' referenced before assignment
2017-03-23 14:46:34 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-23 14:46:34 [guangdong] INFO: get_webpage_count:90
2017-03-23 14:46:34 [guangdong] INFO: success
2017-03-23 14:46:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 181245,
 'downloader/request_count': 302,
 'downloader/request_method_count/GET': 302,
 'downloader/response_bytes': 6512227,
 'downloader/response_count': 302,
 'downloader/response_status_count/200': 302,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 23, 6, 46, 34, 535924),
 'item_scraped_count': 90,
 'log_count/ERROR': 211,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'request_depth_max': 1,
 'response_received_count': 302,
 'scheduler/dequeued': 302,
 'scheduler/dequeued/memory': 302,
 'scheduler/enqueued': 302,
 'scheduler/enqueued/memory': 302,
 'start_time': datetime.datetime(2017, 3, 23, 6, 45, 38, 574794)}
2017-03-23 14:46:34 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-23 14:47:28 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 14:47:28 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 14:47:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 14:47:28 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:47:28 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:47:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 14:47:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 14:47:28 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 14:47:28 [scrapy.core.engine] INFO: Spider opened
2017-03-23 14:47:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 14:47:43 [guangdong] ERROR: save_time_error:time data ' ' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:47:43 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:47:44 [guangdong] ERROR: save_time_error:time data ' ' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:47:44 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:47:45 [guangdong] ERROR: save_time_error:time data ' ' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:47:45 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:47:52 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-23 14:47:52 [guangdong] INFO: get_webpage_count:298
2017-03-23 14:47:52 [guangdong] INFO: success
2017-03-23 14:47:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 181245,
 'downloader/request_count': 302,
 'downloader/request_method_count/GET': 302,
 'downloader/response_bytes': 6512227,
 'downloader/response_count': 302,
 'downloader/response_status_count/200': 302,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 23, 6, 47, 52, 364781),
 'item_scraped_count': 298,
 'log_count/ERROR': 6,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'request_depth_max': 1,
 'response_received_count': 302,
 'scheduler/dequeued': 302,
 'scheduler/dequeued/memory': 302,
 'scheduler/enqueued': 302,
 'scheduler/enqueued/memory': 302,
 'start_time': datetime.datetime(2017, 3, 23, 6, 47, 28, 781851)}
2017-03-23 14:47:52 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-23 14:48:56 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 14:48:56 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 14:48:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 14:48:56 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:48:56 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:48:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 14:48:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 14:48:56 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 14:48:56 [scrapy.core.engine] INFO: Spider opened
2017-03-23 14:48:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 14:49:07 [guangdong] ERROR: save_time_error:time data ' ' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:49:07 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:49:07 [guangdong] ERROR: save_time_error:time data ' ' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:49:07 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:49:09 [guangdong] ERROR: save_time_error:time data ' ' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:49:09 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:49:16 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-23 14:49:16 [guangdong] INFO: get_webpage_count:298
2017-03-23 14:49:16 [guangdong] INFO: success
2017-03-23 14:49:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 181305,
 'downloader/request_count': 302,
 'downloader/request_method_count/GET': 302,
 'downloader/response_bytes': 6512227,
 'downloader/response_count': 302,
 'downloader/response_status_count/200': 302,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 23, 6, 49, 16, 765125),
 'item_scraped_count': 298,
 'log_count/ERROR': 6,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'request_depth_max': 1,
 'response_received_count': 302,
 'scheduler/dequeued': 302,
 'scheduler/dequeued/memory': 302,
 'scheduler/enqueued': 302,
 'scheduler/enqueued/memory': 302,
 'start_time': datetime.datetime(2017, 3, 23, 6, 48, 56, 401820)}
2017-03-23 14:49:16 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-23 14:51:23 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 14:51:23 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 14:51:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 14:51:24 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:51:24 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:51:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 14:51:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 14:51:24 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 14:51:24 [scrapy.core.engine] INFO: Spider opened
2017-03-23 14:51:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 14:52:23 [guangdong] ERROR: save_time_error:time data ' ' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:52:23 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:52:24 [scrapy.extensions.logstats] INFO: Crawled 156 pages (at 156 pages/min), scraped 154 items (at 154 items/min)
2017-03-23 14:52:25 [guangdong] ERROR: save_time_error:time data ' ' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:52:25 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:52:36 [guangdong] ERROR: save_time_error:time data ' ' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:52:36 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:52:58 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-23 14:52:58 [guangdong] INFO: get_webpage_count:298
2017-03-23 14:52:58 [guangdong] INFO: success
2017-03-23 14:52:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 181281,
 'downloader/request_count': 302,
 'downloader/request_method_count/GET': 302,
 'downloader/response_bytes': 6512423,
 'downloader/response_count': 302,
 'downloader/response_status_count/200': 302,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 23, 6, 52, 58, 362248),
 'item_scraped_count': 298,
 'log_count/ERROR': 6,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'request_depth_max': 1,
 'response_received_count': 302,
 'scheduler/dequeued': 302,
 'scheduler/dequeued/memory': 302,
 'scheduler/enqueued': 302,
 'scheduler/enqueued/memory': 302,
 'start_time': datetime.datetime(2017, 3, 23, 6, 51, 24, 251686)}
2017-03-23 14:52:58 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-23 14:57:03 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-23 14:57:03 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-23 14:57:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-23 14:57:03 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:57:03 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-23 14:57:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-23 14:57:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-23 14:57:04 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-23 14:57:04 [scrapy.core.engine] INFO: Spider opened
2017-03-23 14:57:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-23 14:57:38 [guangdong] ERROR: save_time_error:time data ' ' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:57:38 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:57:42 [guangdong] ERROR: save_time_error:time data ' ' does not match format '%Y年%m月%d日 %H:%M'
2017-03-23 14:57:42 [guangdong] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-23 14:58:04 [scrapy.extensions.logstats] INFO: Crawled 289 pages (at 289 pages/min), scraped 285 items (at 285 items/min)
2017-03-23 14:58:08 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-23 14:58:08 [guangdong] INFO: get_webpage_count:299
2017-03-23 14:58:08 [guangdong] INFO: success
2017-03-23 14:58:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 181275,
 'downloader/request_count': 302,
 'downloader/request_method_count/GET': 302,
 'downloader/response_bytes': 6512227,
 'downloader/response_count': 302,
 'downloader/response_status_count/200': 302,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 23, 6, 58, 8, 762656),
 'item_scraped_count': 299,
 'log_count/ERROR': 4,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'request_depth_max': 1,
 'response_received_count': 302,
 'scheduler/dequeued': 302,
 'scheduler/dequeued/memory': 302,
 'scheduler/enqueued': 302,
 'scheduler/enqueued/memory': 302,
 'start_time': datetime.datetime(2017, 3, 23, 6, 57, 4, 176456)}
2017-03-23 14:58:08 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-24 13:52:25 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-24 13:52:25 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-24 13:52:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-24 13:52:25 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 13:52:25 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 13:52:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-24 13:52:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-24 13:52:25 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-24 13:52:25 [scrapy.core.engine] INFO: Spider opened
2017-03-24 13:52:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 13:52:38 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-24 13:52:38 [heilongjiang] INFO: get_webpage_count:0
2017-03-24 13:52:38 [heilongjiang] INFO: success
2017-03-24 13:52:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 20017,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 24, 5, 52, 38, 32170),
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 3, 24, 5, 52, 25, 334727)}
2017-03-24 13:52:38 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-24 13:52:49 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-24 13:52:49 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-24 13:52:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-24 13:52:49 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 13:52:49 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 13:52:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-24 13:52:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-24 13:52:49 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-24 13:52:49 [scrapy.core.engine] INFO: Spider opened
2017-03-24 13:52:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 13:53:03 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-24 13:53:03 [heilongjiang] INFO: get_webpage_count:0
2017-03-24 13:53:03 [heilongjiang] INFO: success
2017-03-24 13:53:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 20017,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 24, 5, 53, 3, 781646),
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 3, 24, 5, 52, 49, 415082)}
2017-03-24 13:53:03 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-24 13:54:50 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-24 13:54:50 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-24 13:54:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-24 13:54:50 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 13:54:50 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 13:54:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-24 13:54:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-24 13:54:51 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-24 13:54:51 [scrapy.core.engine] INFO: Spider opened
2017-03-24 13:54:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 13:55:03 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-24 13:55:03 [heilongjiang] INFO: get_webpage_count:0
2017-03-24 13:55:03 [heilongjiang] INFO: success
2017-03-24 13:55:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 20017,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 24, 5, 55, 3, 86038),
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 3, 24, 5, 54, 51, 55308)}
2017-03-24 13:55:03 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-24 14:01:42 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-24 14:01:42 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-24 14:01:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-24 14:01:42 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 14:01:42 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 14:01:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-24 14:01:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-24 14:01:43 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-24 14:01:43 [scrapy.core.engine] INFO: Spider opened
2017-03-24 14:01:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 14:01:56 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-24 14:01:56 [heilongjiang] INFO: get_webpage_count:0
2017-03-24 14:01:56 [heilongjiang] INFO: success
2017-03-24 14:01:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 20017,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 24, 6, 1, 56, 121939),
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 3, 24, 6, 1, 43, 618387)}
2017-03-24 14:01:56 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-24 16:09:38 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-24 16:09:38 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-24 16:09:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-24 16:09:38 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 16:09:38 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 16:09:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-24 16:09:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-24 16:09:38 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-24 16:09:38 [scrapy.core.engine] INFO: Spider opened
2017-03-24 16:09:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 16:10:47 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-24 16:10:47 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-24 16:10:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 16:10:50 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-24 16:10:50 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-24 16:10:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-24 16:10:50 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 16:10:50 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 16:10:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-24 16:10:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-24 16:10:50 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-24 16:10:50 [scrapy.core.engine] INFO: Spider opened
2017-03-24 16:10:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 16:11:28 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-24 16:11:28 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-24 16:11:37 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-24 16:11:37 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-24 16:11:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-24 16:11:37 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 16:11:37 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 16:11:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-24 16:11:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-24 16:11:37 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-24 16:11:37 [scrapy.core.engine] INFO: Spider opened
2017-03-24 16:11:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 16:11:53 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-24 16:11:53 [heilongjiang] INFO: get_webpage_count:0
2017-03-24 16:11:53 [heilongjiang] INFO: success
2017-03-24 16:11:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/response_bytes': 20224,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 24, 8, 11, 53, 119969),
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 3, 24, 8, 11, 37, 877907)}
2017-03-24 16:11:53 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-24 16:52:33 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-24 16:52:33 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-24 16:52:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-24 16:52:33 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 16:52:33 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 16:52:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-24 16:52:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-24 16:52:33 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-24 16:52:33 [scrapy.core.engine] INFO: Spider opened
2017-03-24 16:52:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 16:54:40 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-24 16:54:40 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 361, in parse
    driver.get("http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court")
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 250, in get
    self.execute(Command.GET, {'url': url})
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 236, in execute
    response = self.command_executor.execute(driver_command, params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 415, in execute
    return self._request(command_info[0], url, body=data)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 489, in _request
    resp = opener.open(request, timeout=self._timeout)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 526, in open
    response = self._open(req, data)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 544, in _open
    '_open', req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 504, in _call_chain
    result = func(*args)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1346, in http_open
    return self.do_open(http.client.HTTPConnection, req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1321, in do_open
    r = h.getresponse()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 1331, in getresponse
    response.begin()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response
2017-03-24 16:54:40 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-24 16:54:40 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 16:54:40 [heilongjiang] INFO: get_webpage_count:0
2017-03-24 16:54:40 [heilongjiang] INFO: success
2017-03-24 16:54:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 461,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 19213,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2017, 3, 24, 8, 54, 40, 790331),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/RemoteDisconnected': 1,
 'start_time': datetime.datetime(2017, 3, 24, 8, 52, 33, 555021)}
2017-03-24 16:54:40 [scrapy.core.engine] INFO: Spider closed (shutdown)
2017-03-24 16:54:52 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-24 16:54:52 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-24 16:54:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-24 16:54:52 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 16:54:52 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 16:54:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-24 16:54:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-24 16:54:52 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-24 16:54:52 [scrapy.core.engine] INFO: Spider opened
2017-03-24 16:54:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 16:56:12 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 375, in parse
    hrefs = origin_html.xpath('//td[@width="200px"]//a[@target="_blank"]/@href').extract()
AttributeError: 'list' object has no attribute 'extract'
2017-03-24 16:56:33 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 16:56:33 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-24 16:56:33 [heilongjiang] INFO: get_webpage_count:0
2017-03-24 16:56:33 [heilongjiang] INFO: success
2017-03-24 16:56:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 457,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 19213,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 24, 8, 56, 33, 967327),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2017, 3, 24, 8, 54, 52, 817276)}
2017-03-24 16:56:33 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-24 16:57:51 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-24 16:57:51 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-24 16:57:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-24 16:57:51 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 16:57:51 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 16:57:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-24 16:57:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-24 16:57:51 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-24 16:57:51 [scrapy.core.engine] INFO: Spider opened
2017-03-24 16:57:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 17:00:03 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-24 17:00:03 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 361, in parse
    driver.get("http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court")
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 250, in get
    self.execute(Command.GET, {'url': url})
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 236, in execute
    response = self.command_executor.execute(driver_command, params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 415, in execute
    return self._request(command_info[0], url, body=data)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 489, in _request
    resp = opener.open(request, timeout=self._timeout)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 526, in open
    response = self._open(req, data)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 544, in _open
    '_open', req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 504, in _call_chain
    result = func(*args)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1346, in http_open
    return self.do_open(http.client.HTTPConnection, req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1321, in do_open
    r = h.getresponse()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 1331, in getresponse
    response.begin()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response
2017-03-24 17:00:03 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-24 17:00:03 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 17:00:03 [heilongjiang] INFO: get_webpage_count:0
2017-03-24 17:00:03 [heilongjiang] INFO: success
2017-03-24 17:00:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 452,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 19213,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2017, 3, 24, 9, 0, 3, 79552),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/RemoteDisconnected': 1,
 'start_time': datetime.datetime(2017, 3, 24, 8, 57, 51, 906431)}
2017-03-24 17:00:03 [scrapy.core.engine] INFO: Spider closed (shutdown)
2017-03-24 17:00:09 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-24 17:00:09 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-24 17:00:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-24 17:00:09 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 17:00:09 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 17:00:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-24 17:00:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-24 17:00:10 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-24 17:00:10 [scrapy.core.engine] INFO: Spider opened
2017-03-24 17:00:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 17:01:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 375, in parse
    hrefs = origin_html.xpath('//td[@width="200px"]//a[@target="_blank"]/@href').extract()
AttributeError: 'list' object has no attribute 'extract'
2017-03-24 17:01:32 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 17:01:32 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-24 17:01:32 [heilongjiang] INFO: get_webpage_count:0
2017-03-24 17:01:32 [heilongjiang] INFO: success
2017-03-24 17:01:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 461,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 19213,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 24, 9, 1, 32, 628898),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2017, 3, 24, 9, 0, 10, 271256)}
2017-03-24 17:01:32 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-24 17:01:41 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-24 17:01:41 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-24 17:01:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-24 17:01:41 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 17:01:41 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 17:01:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-24 17:01:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-24 17:01:41 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-24 17:01:41 [scrapy.core.engine] INFO: Spider opened
2017-03-24 17:01:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 17:01:57 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-24 17:01:58 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 361, in parse
    driver.get("http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court")
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 250, in get
    self.execute(Command.GET, {'url': url})
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 236, in execute
    response = self.command_executor.execute(driver_command, params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 415, in execute
    return self._request(command_info[0], url, body=data)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 489, in _request
    resp = opener.open(request, timeout=self._timeout)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 526, in open
    response = self._open(req, data)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 544, in _open
    '_open', req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 504, in _call_chain
    result = func(*args)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1346, in http_open
    return self.do_open(http.client.HTTPConnection, req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1321, in do_open
    r = h.getresponse()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 1331, in getresponse
    response.begin()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response
2017-03-24 17:01:58 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-24 17:01:58 [heilongjiang] INFO: get_webpage_count:0
2017-03-24 17:01:58 [heilongjiang] INFO: success
2017-03-24 17:01:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 455,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 19213,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2017, 3, 24, 9, 1, 58, 25154),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/RemoteDisconnected': 1,
 'start_time': datetime.datetime(2017, 3, 24, 9, 1, 41, 916843)}
2017-03-24 17:01:58 [scrapy.core.engine] INFO: Spider closed (shutdown)
2017-03-24 17:02:04 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-24 17:02:04 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-24 17:02:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-24 17:02:04 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 17:02:04 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 17:02:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-24 17:02:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-24 17:02:04 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-24 17:02:04 [scrapy.core.engine] INFO: Spider opened
2017-03-24 17:02:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 17:03:28 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 17:04:23 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-24 17:29:04 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-24 17:29:04 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-24 17:29:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-24 17:29:04 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 17:29:04 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-24 17:29:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-24 17:29:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-24 17:29:04 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-24 17:29:04 [scrapy.core.engine] INFO: Spider opened
2017-03-24 17:29:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-24 17:29:20 [heilongjiang] ERROR: save_time_error:time data '<td></td>' does not match format '%Y年%m月%d日'
2017-03-24 17:29:20 [heilongjiang] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-24 17:29:20 [heilongjiang] ERROR: save_time_error:time data '<td></td>' does not match format '%Y年%m月%d日'
2017-03-24 17:29:20 [heilongjiang] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-24 17:29:20 [heilongjiang] ERROR: save_time_error:time data '<td></td>' does not match format '%Y年%m月%d日'
2017-03-24 17:29:20 [heilongjiang] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-24 17:29:20 [heilongjiang] ERROR: save_time_error:time data '<td></td>' does not match format '%Y年%m月%d日'
2017-03-24 17:29:20 [heilongjiang] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-24 17:29:20 [heilongjiang] ERROR: save_time_error:time data '<td></td>' does not match format '%Y年%m月%d日'
2017-03-24 17:29:20 [heilongjiang] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-24 17:29:20 [heilongjiang] ERROR: save_time_error:time data '<td></td>' does not match format '%Y年%m月%d日'
2017-03-24 17:29:20 [heilongjiang] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-24 17:29:21 [heilongjiang] ERROR: save_time_error:time data '<td></td>' does not match format '%Y年%m月%d日'
2017-03-24 17:29:21 [heilongjiang] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-24 17:29:25 [heilongjiang] ERROR: save_time_error:time data '<td></td>' does not match format '%Y年%m月%d日'
2017-03-24 17:29:25 [heilongjiang] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-24 17:29:25 [heilongjiang] ERROR: save_time_error:time data '<td></td>' does not match format '%Y年%m月%d日'
2017-03-24 17:29:25 [heilongjiang] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-24 17:29:26 [heilongjiang] ERROR: save_time_error:time data '<td></td>' does not match format '%Y年%m月%d日'
2017-03-24 17:29:26 [heilongjiang] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-24 17:29:29 [heilongjiang] ERROR: save_time_error:time data '<td></td>' does not match format '%Y年%m月%d日'
2017-03-24 17:29:29 [heilongjiang] ERROR: parse_webpage_error:local variable 'time_get' referenced before assignment
2017-03-24 17:29:29 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-24 17:29:29 [heilongjiang] INFO: get_webpage_count:0
2017-03-24 17:29:29 [heilongjiang] INFO: success
2017-03-24 17:29:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7828,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 104485,
 'downloader/response_count': 12,
 'downloader/response_status_count/200': 12,
 'dupefilter/filtered': 22,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 24, 9, 29, 29, 182620),
 'log_count/ERROR': 22,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'request_depth_max': 1,
 'response_received_count': 12,
 'scheduler/dequeued': 12,
 'scheduler/dequeued/memory': 12,
 'scheduler/enqueued': 12,
 'scheduler/enqueued/memory': 12,
 'start_time': datetime.datetime(2017, 3, 24, 9, 29, 4, 381484)}
2017-03-24 17:29:29 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 10:52:41 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 10:52:41 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 10:52:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 10:52:41 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 10:52:41 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 10:52:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 10:52:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 10:52:41 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 10:52:41 [scrapy.core.engine] INFO: Spider opened
2017-03-27 10:52:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 10:52:42 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-27 10:52:42 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 10:52:42 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 10:52:42 [heilongjiang] INFO: success
2017-03-27 10:52:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1356,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 2, 52, 42, 974546),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 2, 52, 41, 400235)}
2017-03-27 10:52:42 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 10:52:47 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 10:52:47 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 10:52:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 10:52:47 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 10:52:47 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 10:52:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 10:52:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 10:52:47 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 10:52:47 [scrapy.core.engine] INFO: Spider opened
2017-03-27 10:52:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 10:52:48 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-27 10:52:49 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 10:52:49 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 10:52:49 [heilongjiang] INFO: success
2017-03-27 10:52:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1359,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 2, 52, 49, 46521),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 2, 52, 47, 584515)}
2017-03-27 10:52:49 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 10:53:53 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 10:53:53 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 10:53:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 10:53:53 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 10:53:53 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 10:53:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 10:53:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 10:53:53 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 10:53:53 [scrapy.core.engine] INFO: Spider opened
2017-03-27 10:53:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 10:53:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-27 10:53:56 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 10:53:56 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 10:53:56 [heilongjiang] INFO: success
2017-03-27 10:53:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1356,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 2, 53, 56, 345115),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 2, 53, 53, 805206)}
2017-03-27 10:53:56 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 10:59:22 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 10:59:22 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 10:59:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 10:59:22 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 10:59:22 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 10:59:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 10:59:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 10:59:23 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 10:59:23 [scrapy.core.engine] INFO: Spider opened
2017-03-27 10:59:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 10:59:25 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-27 10:59:25 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 10:59:25 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 10:59:25 [heilongjiang] INFO: success
2017-03-27 10:59:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1269,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 2, 59, 25, 913155),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 2, 59, 23, 127815)}
2017-03-27 10:59:25 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 11:02:52 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 11:02:52 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 11:02:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 11:02:53 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:02:53 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:02:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 11:02:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 11:02:53 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 11:02:53 [scrapy.core.engine] INFO: Spider opened
2017-03-27 11:02:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 11:02:53 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 11:02:53 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 11:02:53 [heilongjiang] INFO: success
2017-03-27 11:02:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 3, 2, 53, 227193),
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'start_time': datetime.datetime(2017, 3, 27, 3, 2, 53, 221608)}
2017-03-27 11:02:53 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 11:03:17 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 11:03:17 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 11:03:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 11:03:17 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:03:17 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:03:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 11:03:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 11:03:17 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 11:03:17 [scrapy.core.engine] INFO: Spider opened
2017-03-27 11:03:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 11:03:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 379, in parse
    driver.find_element_by_xpath(href).click()
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 295, in find_element_by_xpath
    return self.find_element(by=By.XPATH, value=xpath)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 756, in find_element
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 238, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 193, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSelectorException: Message: {"errorMessage":"Unable to locate an element with the xpath expression CtrlServlet?parm=view_Court&of040=118100000142&of20305=20170328（09）时（00）分 because of the following error:\nError: INVALID_EXPRESSION_ERR: DOM XPath Exception 51","request":{"headers":{"Accept":"application/json","Accept-Encoding":"identity","Connection":"close","Content-Length":"187","Content-Type":"application/json;charset=UTF-8","Host":"127.0.0.1:55067","User-Agent":"Python-urllib/3.6"},"httpVersion":"1.1","method":"POST","post":"{\"using\": \"xpath\", \"value\": \"CtrlServlet?parm=view_Court&of040=118100000142&of20305=20170328\\uff0809\\uff09\\u65f6\\uff0800\\uff09\\u5206\", \"sessionId\": \"ef2512d0-1299-11e7-95ce-3120ce62c2f3\"}","url":"/element","urlParsed":{"anchor":"","query":"","file":"element","directory":"/","path":"/element","relative":"/element","port":"","host":"","password":"","user":"","userInfo":"","authority":"","protocol":"","source":"/element","queryKey":{},"chunks":["element"]},"urlOriginal":"/session/ef2512d0-1299-11e7-95ce-3120ce62c2f3/element"}}
Screenshot: available via screen

2017-03-27 11:03:31 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 11:03:31 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 11:03:31 [heilongjiang] INFO: success
2017-03-27 11:03:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 443,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79087,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 3, 3, 31, 614161),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/InvalidSelectorException': 1,
 'start_time': datetime.datetime(2017, 3, 27, 3, 3, 17, 522555)}
2017-03-27 11:03:31 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 11:04:06 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 11:04:06 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 11:04:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 11:04:06 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:04:06 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:04:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 11:04:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 11:04:06 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 11:04:06 [scrapy.core.engine] INFO: Spider opened
2017-03-27 11:04:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 11:04:25 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 379, in parse
    driver.find_element_by_xpath(href).click()
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 295, in find_element_by_xpath
    return self.find_element(by=By.XPATH, value=xpath)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 756, in find_element
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 238, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 193, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSelectorException: Message: {"errorMessage":"Unable to locate an element with the xpath expression CtrlServlet?parm=view_Court&of040=118100000142&of20305=20170328（09）时（00）分 because of the following error:\nError: INVALID_EXPRESSION_ERR: DOM XPath Exception 51","request":{"headers":{"Accept":"application/json","Accept-Encoding":"identity","Connection":"close","Content-Length":"187","Content-Type":"application/json;charset=UTF-8","Host":"127.0.0.1:55308","User-Agent":"Python-urllib/3.6"},"httpVersion":"1.1","method":"POST","post":"{\"using\": \"xpath\", \"value\": \"CtrlServlet?parm=view_Court&of040=118100000142&of20305=20170328\\uff0809\\uff09\\u65f6\\uff0800\\uff09\\u5206\", \"sessionId\": \"0ba0a370-129a-11e7-8b80-dd48b4aae373\"}","url":"/element","urlParsed":{"anchor":"","query":"","file":"element","directory":"/","path":"/element","relative":"/element","port":"","host":"","password":"","user":"","userInfo":"","authority":"","protocol":"","source":"/element","queryKey":{},"chunks":["element"]},"urlOriginal":"/session/0ba0a370-129a-11e7-8b80-dd48b4aae373/element"}}
Screenshot: available via screen

2017-03-27 11:04:33 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 11:04:33 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 11:04:33 [heilongjiang] INFO: success
2017-03-27 11:04:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 445,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79087,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 3, 4, 33, 94573),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/InvalidSelectorException': 1,
 'start_time': datetime.datetime(2017, 3, 27, 3, 4, 6, 712063)}
2017-03-27 11:04:33 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 11:28:14 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 11:28:14 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 11:28:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 11:28:14 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:28:14 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:28:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 11:28:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 11:28:14 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 11:28:14 [scrapy.core.engine] INFO: Spider opened
2017-03-27 11:28:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 11:28:16 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-27 11:28:16 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 11:28:16 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 11:28:16 [heilongjiang] INFO: success
2017-03-27 11:28:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1308,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 3, 28, 16, 775189),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 3, 28, 14, 737827)}
2017-03-27 11:28:16 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 11:28:35 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 11:28:35 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 11:28:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 11:28:35 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:28:35 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:28:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 11:28:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 11:28:36 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 11:28:36 [scrapy.core.engine] INFO: Spider opened
2017-03-27 11:28:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 11:28:39 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-27 11:28:39 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 11:28:39 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 11:28:39 [heilongjiang] INFO: success
2017-03-27 11:28:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1329,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 3, 28, 39, 677236),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 3, 28, 36, 157939)}
2017-03-27 11:28:39 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 11:30:22 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 11:30:22 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 11:30:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 11:30:22 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:30:22 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:30:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 11:30:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 11:30:22 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 11:30:22 [scrapy.core.engine] INFO: Spider opened
2017-03-27 11:30:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 11:30:23 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-27 11:30:23 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 11:30:23 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 11:30:23 [heilongjiang] INFO: success
2017-03-27 11:30:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1308,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 3, 30, 23, 990864),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 3, 30, 22, 273791)}
2017-03-27 11:30:23 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 11:30:26 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 11:30:26 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 11:30:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 11:30:26 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:30:26 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:30:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 11:30:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 11:30:27 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 11:30:27 [scrapy.core.engine] INFO: Spider opened
2017-03-27 11:30:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 11:30:28 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-27 11:30:28 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 11:30:28 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 11:30:28 [heilongjiang] INFO: success
2017-03-27 11:30:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1329,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 3, 30, 28, 475807),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 3, 30, 27, 8768)}
2017-03-27 11:30:28 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 11:31:04 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 11:31:04 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 11:31:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 11:31:04 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:31:04 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 11:31:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 11:31:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 11:31:04 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 11:31:04 [scrapy.core.engine] INFO: Spider opened
2017-03-27 11:31:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 11:31:31 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-27 11:31:31 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-27 13:07:11 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 13:07:11 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 13:07:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 13:07:11 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:07:11 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:07:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 13:07:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 13:07:11 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 13:07:11 [scrapy.core.engine] INFO: Spider opened
2017-03-27 13:07:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 13:08:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 13:09:02 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-27 13:09:02 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 13:09:02 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 13:09:02 [heilongjiang] INFO: success
2017-03-27 13:09:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 2,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 1,
 'downloader/request_bytes': 1329,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 5, 9, 2, 386937),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 5, 7, 11, 294872)}
2017-03-27 13:09:02 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 13:30:29 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 13:30:29 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 13:30:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 13:30:29 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:30:29 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:30:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 13:30:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 13:30:29 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 13:30:29 [scrapy.core.engine] INFO: Spider opened
2017-03-27 13:30:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 13:30:29 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-27 13:30:29 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 13:30:29 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 13:30:29 [heilongjiang] INFO: success
2017-03-27 13:30:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1308,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 5, 30, 29, 738876),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 5, 30, 29, 319053)}
2017-03-27 13:30:29 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 13:31:38 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 13:31:38 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 13:31:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 13:31:38 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:31:38 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:31:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 13:31:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 13:31:38 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 13:31:38 [scrapy.core.engine] INFO: Spider opened
2017-03-27 13:31:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 13:31:38 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-27 13:31:38 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 13:31:38 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 13:31:38 [heilongjiang] INFO: success
2017-03-27 13:31:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1329,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 5, 31, 38, 701894),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 5, 31, 38, 313515)}
2017-03-27 13:31:38 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 13:31:50 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 13:31:50 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 13:31:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 13:31:50 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:31:50 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:31:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 13:31:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 13:31:51 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 13:31:51 [scrapy.core.engine] INFO: Spider opened
2017-03-27 13:31:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 13:31:51 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-27 13:31:51 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 13:31:51 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 13:31:51 [heilongjiang] INFO: success
2017-03-27 13:31:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1308,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 5, 31, 51, 436481),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 5, 31, 51, 21293)}
2017-03-27 13:31:51 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 13:32:52 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 13:32:52 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 13:32:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 13:32:52 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:32:52 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:32:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 13:32:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 13:32:53 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 13:32:53 [scrapy.core.engine] INFO: Spider opened
2017-03-27 13:32:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 13:32:53 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-27 13:32:53 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 13:32:53 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 13:32:53 [heilongjiang] INFO: success
2017-03-27 13:32:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1485,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 5, 32, 53, 452682),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 5, 32, 53, 15667)}
2017-03-27 13:32:53 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 13:33:22 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 13:33:22 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 13:33:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 13:33:22 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:33:22 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:33:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 13:33:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 13:33:22 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 13:33:22 [scrapy.core.engine] INFO: Spider opened
2017-03-27 13:33:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 13:33:22 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-27 13:33:22 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 13:33:22 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 13:33:22 [heilongjiang] INFO: success
2017-03-27 13:33:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1485,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 5, 33, 22, 882026),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 5, 33, 22, 462738)}
2017-03-27 13:33:22 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 13:33:35 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 13:33:35 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 13:33:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 13:33:35 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:33:35 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:33:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 13:33:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 13:33:35 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 13:33:35 [scrapy.core.engine] INFO: Spider opened
2017-03-27 13:33:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 13:33:38 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-27 13:33:38 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 13:33:38 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 13:33:38 [heilongjiang] INFO: success
2017-03-27 13:33:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1464,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 5, 33, 38, 902898),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 5, 33, 35, 699979)}
2017-03-27 13:33:38 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 13:33:42 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 13:33:42 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 13:33:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 13:33:42 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:33:42 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:33:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 13:33:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 13:33:42 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 13:33:42 [scrapy.core.engine] INFO: Spider opened
2017-03-27 13:33:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 13:33:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-27 13:33:46 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 13:33:46 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 13:33:46 [heilongjiang] INFO: success
2017-03-27 13:33:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1479,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 5, 33, 46, 193715),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 5, 33, 42, 991592)}
2017-03-27 13:33:46 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 13:34:09 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 13:34:09 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 13:34:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 13:34:09 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:34:09 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:34:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 13:34:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 13:34:09 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 13:34:09 [scrapy.core.engine] INFO: Spider opened
2017-03-27 13:34:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 13:34:12 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-27 13:34:12 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 13:34:12 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 13:34:12 [heilongjiang] INFO: success
2017-03-27 13:34:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1401,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 5, 34, 12, 246135),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 5, 34, 9, 319236)}
2017-03-27 13:34:12 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 13:37:15 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 13:37:15 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 13:37:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 13:37:15 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:37:15 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:37:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 13:37:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 13:37:15 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 13:37:15 [scrapy.core.engine] INFO: Spider opened
2017-03-27 13:37:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 13:37:20 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-27 13:37:20 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 13:37:20 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 13:37:20 [heilongjiang] INFO: success
2017-03-27 13:37:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1401,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 5, 37, 20, 187448),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 5, 37, 15, 778720)}
2017-03-27 13:37:20 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 13:37:35 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 13:37:35 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 13:37:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 13:37:35 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:37:35 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:37:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 13:37:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 13:37:35 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 13:37:35 [scrapy.core.engine] INFO: Spider opened
2017-03-27 13:37:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 13:37:38 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-27 13:37:38 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 13:37:38 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 13:37:38 [heilongjiang] INFO: success
2017-03-27 13:37:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1422,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 5, 37, 38, 548246),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 5, 37, 35, 240919)}
2017-03-27 13:37:38 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 13:37:46 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 13:37:46 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 13:37:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 13:37:46 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:37:46 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 13:37:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 13:37:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 13:37:46 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 13:37:46 [scrapy.core.engine] INFO: Spider opened
2017-03-27 13:37:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 13:37:50 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-27 13:37:50 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 13:37:50 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 13:37:50 [heilongjiang] INFO: success
2017-03-27 13:37:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1404,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 5, 37, 50, 392121),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 5, 37, 46, 846403)}
2017-03-27 13:37:50 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 17:33:12 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 17:33:12 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 17:33:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 17:33:12 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 17:33:12 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 17:33:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 17:33:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 17:33:12 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 17:33:12 [scrapy.core.engine] INFO: Spider opened
2017-03-27 17:33:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 17:33:12 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-27 17:33:13 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 17:33:13 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 17:33:13 [heilongjiang] INFO: success
2017-03-27 17:33:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1266,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 9, 33, 13, 88250),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 9, 33, 12, 639869)}
2017-03-27 17:33:13 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 17:33:23 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 17:33:23 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 17:33:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 17:33:23 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 17:33:23 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 17:33:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 17:33:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 17:33:24 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 17:33:24 [scrapy.core.engine] INFO: Spider opened
2017-03-27 17:33:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 17:33:24 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-27 17:33:24 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 17:33:24 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 17:33:24 [heilongjiang] INFO: success
2017-03-27 17:33:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1266,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 9, 33, 24, 575402),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 27, 9, 33, 24, 155805)}
2017-03-27 17:33:24 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 17:36:26 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 17:36:26 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 17:36:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 17:36:26 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 17:36:26 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 17:36:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 17:36:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 17:36:26 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 17:36:26 [scrapy.core.engine] INFO: Spider opened
2017-03-27 17:36:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 17:36:38 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 379, in parse
    driver.find_element_by_link_text(text_name).click()
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 319, in find_element_by_link_text
    return self.find_element(by=By.LINK_TEXT, value=link_text)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 756, in find_element
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 238, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 193, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: {"errorMessage":"Unable to find element with link text '北劳人仲字〔2017〕第2号 '","request":{"headers":{"Accept":"application/json","Accept-Encoding":"identity","Connection":"close","Content-Length":"153","Content-Type":"application/json;charset=UTF-8","Host":"127.0.0.1:56306","User-Agent":"Python-urllib/3.6"},"httpVersion":"1.1","method":"POST","post":"{\"using\": \"link text\", \"value\": \"\\u5317\\u52b3\\u4eba\\u4ef2\\u5b57\\u30142017\\u3015\\u7b2c2\\u53f7\\u00a0\", \"sessionId\": \"db1c9330-12d0-11e7-9456-b5d5f05de8fb\"}","url":"/element","urlParsed":{"anchor":"","query":"","file":"element","directory":"/","path":"/element","relative":"/element","port":"","host":"","password":"","user":"","userInfo":"","authority":"","protocol":"","source":"/element","queryKey":{},"chunks":["element"]},"urlOriginal":"/session/db1c9330-12d0-11e7-9456-b5d5f05de8fb/element"}}
Screenshot: available via screen

2017-03-27 17:36:38 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 17:36:38 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 17:36:38 [heilongjiang] INFO: success
2017-03-27 17:36:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 415,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79087,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 9, 36, 38, 449406),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchElementException': 1,
 'start_time': datetime.datetime(2017, 3, 27, 9, 36, 26, 466238)}
2017-03-27 17:36:38 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 17:37:28 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 17:37:28 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 17:37:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 17:37:28 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 17:37:28 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 17:37:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 17:37:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 17:37:29 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 17:37:29 [scrapy.core.engine] INFO: Spider opened
2017-03-27 17:37:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 17:38:15 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 379, in parse
    driver.find_element_by_link_text(text_name).click()
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 319, in find_element_by_link_text
    return self.find_element(by=By.LINK_TEXT, value=link_text)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 756, in find_element
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 238, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 193, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: {"errorMessage":"Unable to find element with link text '北劳人仲字〔2017〕第2号 '","request":{"headers":{"Accept":"application/json","Accept-Encoding":"identity","Connection":"close","Content-Length":"153","Content-Type":"application/json;charset=UTF-8","Host":"127.0.0.1:56590","User-Agent":"Python-urllib/3.6"},"httpVersion":"1.1","method":"POST","post":"{\"using\": \"link text\", \"value\": \"\\u5317\\u52b3\\u4eba\\u4ef2\\u5b57\\u30142017\\u3015\\u7b2c2\\u53f7\\u00a0\", \"sessionId\": \"01d9b840-12d1-11e7-90b8-f58e794744eb\"}","url":"/element","urlParsed":{"anchor":"","query":"","file":"element","directory":"/","path":"/element","relative":"/element","port":"","host":"","password":"","user":"","userInfo":"","authority":"","protocol":"","source":"/element","queryKey":{},"chunks":["element"]},"urlOriginal":"/session/01d9b840-12d1-11e7-90b8-f58e794744eb/element"}}
Screenshot: available via screen

2017-03-27 17:38:30 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 17:38:30 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 17:38:30 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 17:38:30 [heilongjiang] INFO: success
2017-03-27 17:38:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 415,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79087,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 9, 38, 30, 506816),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchElementException': 1,
 'start_time': datetime.datetime(2017, 3, 27, 9, 37, 29, 126065)}
2017-03-27 17:38:30 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 17:41:11 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 17:41:11 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 17:41:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 17:41:11 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 17:41:11 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 17:41:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 17:41:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 17:41:11 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 17:41:11 [scrapy.core.engine] INFO: Spider opened
2017-03-27 17:41:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 17:45:18 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-27 17:45:30 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 17:45:30 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 17:45:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 17:45:30 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 17:45:30 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 17:45:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 17:45:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 17:45:30 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 17:45:30 [scrapy.core.engine] INFO: Spider opened
2017-03-27 17:45:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 17:45:54 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 380, in parse
    driver.find_element_by_link_text(text).click()
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 319, in find_element_by_link_text
    return self.find_element(by=By.LINK_TEXT, value=link_text)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 756, in find_element
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 238, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 193, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: {"errorMessage":"Unable to find element with link text '北劳人仲字〔2017〕第2号 '","request":{"headers":{"Accept":"application/json","Accept-Encoding":"identity","Connection":"close","Content-Length":"153","Content-Type":"application/json;charset=UTF-8","Host":"127.0.0.1:58681","User-Agent":"Python-urllib/3.6"},"httpVersion":"1.1","method":"POST","post":"{\"using\": \"link text\", \"value\": \"\\u5317\\u52b3\\u4eba\\u4ef2\\u5b57\\u30142017\\u3015\\u7b2c2\\u53f7\\u00a0\", \"sessionId\": \"1f9e9c50-12d2-11e7-9876-c9c4e0bcb7ed\"}","url":"/element","urlParsed":{"anchor":"","query":"","file":"element","directory":"/","path":"/element","relative":"/element","port":"","host":"","password":"","user":"","userInfo":"","authority":"","protocol":"","source":"/element","queryKey":{},"chunks":["element"]},"urlOriginal":"/session/1f9e9c50-12d2-11e7-9876-c9c4e0bcb7ed/element"}}
Screenshot: available via screen

2017-03-27 17:46:07 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-27 17:46:07 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 17:46:07 [heilongjiang] INFO: success
2017-03-27 17:46:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 421,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79087,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 27, 9, 46, 7, 478590),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchElementException': 1,
 'start_time': datetime.datetime(2017, 3, 27, 9, 45, 31, 3250)}
2017-03-27 17:46:07 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-27 17:53:08 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-27 17:53:08 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-27 17:53:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-27 17:53:08 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 17:53:08 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-27 17:53:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-27 17:53:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-27 17:53:08 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-27 17:53:08 [scrapy.core.engine] INFO: Spider opened
2017-03-27 17:53:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 20:00:47 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-27 20:00:54 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp> (referer: None)
Traceback (most recent call last):
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1318, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 964, in send
    self.connect()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 936, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/socket.py", line 722, in create_connection
    raise err
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/socket.py", line 713, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 380, in parse
    driver.find_element_by_link_text(text).click()
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 319, in find_element_by_link_text
    return self.find_element(by=By.LINK_TEXT, value=link_text)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 756, in find_element
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 236, in execute
    response = self.command_executor.execute(driver_command, params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 415, in execute
    return self._request(command_info[0], url, body=data)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 489, in _request
    resp = opener.open(request, timeout=self._timeout)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 526, in open
    response = self._open(req, data)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 544, in _open
    '_open', req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 504, in _call_chain
    result = func(*args)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1346, in http_open
    return self.do_open(http.client.HTTPConnection, req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1320, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno 61] Connection refused>
2017-03-27 20:00:54 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-27 20:00:54 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-27 20:00:54 [heilongjiang] INFO: get_webpage_count:0
2017-03-27 20:00:54 [heilongjiang] INFO: success
2017-03-27 20:00:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 422,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79087,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2017, 3, 27, 12, 0, 54, 743210),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/URLError': 1,
 'start_time': datetime.datetime(2017, 3, 27, 9, 53, 8, 990864)}
2017-03-27 20:00:54 [scrapy.core.engine] INFO: Spider closed (shutdown)
2017-03-28 10:35:19 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 10:35:19 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 10:35:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 10:35:19 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 10:35:19 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 10:35:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 10:35:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 10:35:19 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 10:35:19 [scrapy.core.engine] INFO: Spider opened
2017-03-28 10:35:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 10:36:54 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 380, in parse
    driver.find_element_by_link_text(text).click()
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 319, in find_element_by_link_text
    return self.find_element(by=By.LINK_TEXT, value=link_text)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 756, in find_element
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 238, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 193, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: {"errorMessage":"Unable to find element with link text '北劳人仲字〔2017〕第2号 '","request":{"headers":{"Accept":"application/json","Accept-Encoding":"identity","Connection":"close","Content-Length":"153","Content-Type":"application/json;charset=UTF-8","Host":"127.0.0.1:61432","User-Agent":"Python-urllib/3.6"},"httpVersion":"1.1","method":"POST","post":"{\"using\": \"link text\", \"value\": \"\\u5317\\u52b3\\u4eba\\u4ef2\\u5b57\\u30142017\\u3015\\u7b2c2\\u53f7\\u00a0\", \"sessionId\": \"4d6cb440-135f-11e7-9393-37b0c3c78b47\"}","url":"/element","urlParsed":{"anchor":"","query":"","file":"element","directory":"/","path":"/element","relative":"/element","port":"","host":"","password":"","user":"","userInfo":"","authority":"","protocol":"","source":"/element","queryKey":{},"chunks":["element"]},"urlOriginal":"/session/4d6cb440-135f-11e7-9393-37b0c3c78b47/element"}}
Screenshot: available via screen

2017-03-28 10:36:54 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 10:36:54 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 10:36:54 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 10:36:54 [heilongjiang] INFO: success
2017-03-28 10:36:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 415,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79087,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 2, 36, 54, 716267),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchElementException': 1,
 'start_time': datetime.datetime(2017, 3, 28, 2, 35, 19, 940733)}
2017-03-28 10:36:54 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 10:37:58 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 10:37:58 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 10:37:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 10:37:59 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 10:37:59 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 10:37:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 10:37:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 10:37:59 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 10:37:59 [scrapy.core.engine] INFO: Spider opened
2017-03-28 10:37:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 10:39:10 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 380, in parse
    driver.find_element_by_link_text(text).click()
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 319, in find_element_by_link_text
    return self.find_element(by=By.LINK_TEXT, value=link_text)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 756, in find_element
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 238, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 193, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: {"errorMessage":"Unable to find element with link text '北劳人仲字〔2017〕第2号 '","request":{"headers":{"Accept":"application/json","Accept-Encoding":"identity","Connection":"close","Content-Length":"153","Content-Type":"application/json;charset=UTF-8","Host":"127.0.0.1:62070","User-Agent":"Python-urllib/3.6"},"httpVersion":"1.1","method":"POST","post":"{\"using\": \"link text\", \"value\": \"\\u5317\\u52b3\\u4eba\\u4ef2\\u5b57\\u30142017\\u3015\\u7b2c2\\u53f7\\u00a0\", \"sessionId\": \"94c9bf40-135f-11e7-95d8-b7d2830da34c\"}","url":"/element","urlParsed":{"anchor":"","query":"","file":"element","directory":"/","path":"/element","relative":"/element","port":"","host":"","password":"","user":"","userInfo":"","authority":"","protocol":"","source":"/element","queryKey":{},"chunks":["element"]},"urlOriginal":"/session/94c9bf40-135f-11e7-95d8-b7d2830da34c/element"}}
Screenshot: available via screen

2017-03-28 10:39:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 10:39:10 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 10:39:10 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 10:39:10 [heilongjiang] INFO: success
2017-03-28 10:39:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 415,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79087,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 2, 39, 10, 691406),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchElementException': 1,
 'start_time': datetime.datetime(2017, 3, 28, 2, 37, 59, 492001)}
2017-03-28 10:39:10 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:32:00 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:32:00 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:32:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:32:00 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:32:00 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:32:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:32:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:32:01 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:32:01 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:32:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:32:47 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 383, in parse
    driver.find_element_by_link_text(text).click()
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 319, in find_element_by_link_text
    return self.find_element(by=By.LINK_TEXT, value=link_text)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 756, in find_element
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 238, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 193, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: {"errorMessage":"Unable to find element with link text '北劳人仲字〔2017〕第2号 '","request":{"headers":{"Accept":"application/json","Accept-Encoding":"identity","Connection":"close","Content-Length":"153","Content-Type":"application/json;charset=UTF-8","Host":"127.0.0.1:65325","User-Agent":"Python-urllib/3.6"},"httpVersion":"1.1","method":"POST","post":"{\"using\": \"link text\", \"value\": \"\\u5317\\u52b3\\u4eba\\u4ef2\\u5b57\\u30142017\\u3015\\u7b2c2\\u53f7\\u00a0\", \"sessionId\": \"a4879b40-1388-11e7-89e7-87c40687a272\"}","url":"/element","urlParsed":{"anchor":"","query":"","file":"element","directory":"/","path":"/element","relative":"/element","port":"","host":"","password":"","user":"","userInfo":"","authority":"","protocol":"","source":"/element","queryKey":{},"chunks":["element"]},"urlOriginal":"/session/a4879b40-1388-11e7-89e7-87c40687a272/element"}}
Screenshot: available via screen

2017-03-28 15:33:03 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:33:03 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:33:03 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:33:03 [heilongjiang] INFO: success
2017-03-28 15:33:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 424,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79087,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 33, 3, 246061),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchElementException': 1,
 'start_time': datetime.datetime(2017, 3, 28, 7, 32, 1, 257730)}
2017-03-28 15:33:03 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:33:42 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:33:42 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:33:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:33:42 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:33:42 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:33:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:33:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:33:43 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:33:43 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:33:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:34:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:34:47 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-28 15:34:47 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-28 15:34:57 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:34:57 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:34:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:34:57 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:34:57 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:34:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:34:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:34:58 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:34:58 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:34:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:35:23 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 383, in parse
    driver.find_element_by_link_text(text).click()
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 319, in find_element_by_link_text
    return self.find_element(by=By.LINK_TEXT, value=link_text)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 756, in find_element
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 238, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 193, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: {"errorMessage":"Unable to find element with link text '北劳人仲字〔2017〕第2号'","request":{"headers":{"Accept":"application/json","Accept-Encoding":"identity","Connection":"close","Content-Length":"147","Content-Type":"application/json;charset=UTF-8","Host":"127.0.0.1:49663","User-Agent":"Python-urllib/3.6"},"httpVersion":"1.1","method":"POST","post":"{\"using\": \"link text\", \"value\": \"\\u5317\\u52b3\\u4eba\\u4ef2\\u5b57\\u30142017\\u3015\\u7b2c2\\u53f7\", \"sessionId\": \"11d07190-1389-11e7-9bfe-0598019e165b\"}","url":"/element","urlParsed":{"anchor":"","query":"","file":"element","directory":"/","path":"/element","relative":"/element","port":"","host":"","password":"","user":"","userInfo":"","authority":"","protocol":"","source":"/element","queryKey":{},"chunks":["element"]},"urlOriginal":"/session/11d07190-1389-11e7-9bfe-0598019e165b/element"}}
Screenshot: available via screen

2017-03-28 15:35:23 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:35:23 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:35:23 [heilongjiang] INFO: success
2017-03-28 15:35:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 415,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79087,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 35, 23, 328851),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchElementException': 1,
 'start_time': datetime.datetime(2017, 3, 28, 7, 34, 58, 78520)}
2017-03-28 15:35:23 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:37:39 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:37:39 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:37:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:37:39 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:37:39 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:37:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:37:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:37:40 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:37:40 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:37:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:41:30 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-28 15:41:30 [scrapy.core.scraper] ERROR: Spider error processing <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 362, in parse
    driver.get("http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court")
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 250, in get
    self.execute(Command.GET, {'url': url})
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 236, in execute
    response = self.command_executor.execute(driver_command, params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 415, in execute
    return self._request(command_info[0], url, body=data)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 489, in _request
    resp = opener.open(request, timeout=self._timeout)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 526, in open
    response = self._open(req, data)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 544, in _open
    '_open', req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 504, in _call_chain
    result = func(*args)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1346, in http_open
    return self.do_open(http.client.HTTPConnection, req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1321, in do_open
    r = h.getresponse()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 1331, in getresponse
    response.begin()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response
2017-03-28 15:41:30 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-28 15:41:30 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:41:30 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:41:30 [heilongjiang] INFO: success
2017-03-28 15:41:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 415,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 79087,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 41, 30, 929532),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/RemoteDisconnected': 1,
 'start_time': datetime.datetime(2017, 3, 28, 7, 37, 40, 596266)}
2017-03-28 15:41:30 [scrapy.core.engine] INFO: Spider closed (shutdown)
2017-03-28 15:41:40 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:41:40 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:41:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:41:40 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:41:40 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:41:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:41:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:41:41 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:41:41 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:41:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:41:50 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 15:41:50 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:41:50 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:41:50 [heilongjiang] INFO: success
2017-03-28 15:41:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1260,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 41, 50, 686981),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 7, 41, 41, 574277)}
2017-03-28 15:41:50 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:42:24 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:42:24 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:42:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:42:24 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:42:24 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:42:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:42:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:42:25 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:42:25 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:42:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:43:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:44:18 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-28 15:44:18 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-28 15:44:28 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:44:28 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:44:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:44:28 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:44:28 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:44:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:44:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:44:29 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:44:29 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:44:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:44:32 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 15:44:32 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:44:32 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:44:32 [heilongjiang] INFO: success
2017-03-28 15:44:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1245,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 44, 32, 887091),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 7, 44, 29, 596251)}
2017-03-28 15:44:32 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:44:46 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:44:46 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:44:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:44:47 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:44:47 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:44:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:44:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:44:47 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:44:47 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:44:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:44:50 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 15:44:50 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:44:50 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:44:50 [heilongjiang] INFO: success
2017-03-28 15:44:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1245,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 44, 50, 379224),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 7, 44, 47, 733051)}
2017-03-28 15:44:50 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:45:28 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:45:28 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:45:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:45:29 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:45:29 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:45:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:45:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:45:30 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:45:30 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:45:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:45:31 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljhrss/indexNew.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 15:45:31 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:45:31 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:45:31 [heilongjiang] INFO: success
2017-03-28 15:45:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1266,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 45, 31, 704380),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 7, 45, 30, 90218)}
2017-03-28 15:45:31 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:45:54 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:45:54 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:45:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:45:54 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:45:54 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:45:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:45:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:45:55 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:45:55 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:45:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:45:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 15:45:56 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:45:56 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:45:56 [heilongjiang] INFO: success
2017-03-28 15:45:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1314,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 45, 56, 757267),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 7, 45, 55, 177636)}
2017-03-28 15:45:56 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:46:08 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:46:08 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:46:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:46:08 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:46:08 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:46:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:46:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:46:08 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:46:08 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:46:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:46:09 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 15:46:10 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:46:10 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:46:10 [heilongjiang] INFO: success
2017-03-28 15:46:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1302,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 46, 10, 69863),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 7, 46, 8, 723097)}
2017-03-28 15:46:10 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:47:45 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:47:45 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:47:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:47:45 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:47:45 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:47:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:47:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:47:46 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:47:46 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:47:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:47:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 15:47:48 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:47:48 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:47:48 [heilongjiang] INFO: success
2017-03-28 15:47:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1314,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 47, 48, 63331),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 7, 47, 46, 675697)}
2017-03-28 15:47:48 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:48:54 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:48:54 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:48:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:48:54 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:48:54 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:48:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:48:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:48:55 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:48:55 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:48:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:48:56 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 15:48:56 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:48:56 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:48:56 [heilongjiang] INFO: success
2017-03-28 15:48:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1383,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 48, 56, 506094),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 7, 48, 55, 237851)}
2017-03-28 15:48:56 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:49:15 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:49:15 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:49:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:49:15 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:49:15 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:49:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:49:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:49:16 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:49:16 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:49:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:49:17 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 15:49:17 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:49:17 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:49:17 [heilongjiang] INFO: success
2017-03-28 15:49:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1293,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 49, 17, 765069),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 7, 49, 16, 332256)}
2017-03-28 15:49:17 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:53:02 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:53:02 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:53:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:53:02 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:53:02 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:53:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:53:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:53:03 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:53:03 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:53:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:53:05 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 15:53:05 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:53:05 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:53:05 [heilongjiang] INFO: success
2017-03-28 15:53:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1224,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 53, 5, 196988),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 7, 53, 3, 608307)}
2017-03-28 15:53:05 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:55:01 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:55:01 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:55:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:55:01 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:55:01 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:55:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:55:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:55:02 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:55:02 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:55:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:55:04 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 15:55:04 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:55:04 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:55:04 [heilongjiang] INFO: success
2017-03-28 15:55:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1086,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 55, 4, 281566),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 7, 55, 2, 576165)}
2017-03-28 15:55:04 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:55:18 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:55:18 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:55:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:55:18 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:55:18 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:55:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:55:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:55:19 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:55:19 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:55:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:55:20 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 15:55:20 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:55:20 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:55:20 [heilongjiang] INFO: success
2017-03-28 15:55:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 825,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 55, 20, 774998),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 7, 55, 19, 275325)}
2017-03-28 15:55:20 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:55:53 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:55:53 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:55:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:55:53 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:55:53 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:55:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:55:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:55:54 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:55:54 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:55:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:55:57 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 15:55:57 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:55:57 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:55:57 [heilongjiang] INFO: success
2017-03-28 15:55:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1356,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 55, 57, 803218),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 7, 55, 54, 678382)}
2017-03-28 15:55:57 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 15:58:05 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 15:58:05 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 15:58:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 15:58:05 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:58:05 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 15:58:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 15:58:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 15:58:06 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 15:58:06 [scrapy.core.engine] INFO: Spider opened
2017-03-28 15:58:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 15:58:09 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 15:58:09 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 15:58:09 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 15:58:09 [heilongjiang] INFO: success
2017-03-28 15:58:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1374,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 7, 58, 9, 432641),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 7, 58, 6, 249983)}
2017-03-28 15:58:09 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:01:18 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:01:18 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:01:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:01:18 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:01:18 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:01:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:01:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:01:18 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:01:18 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:01:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:01:21 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 16:01:21 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:01:21 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:01:21 [heilongjiang] INFO: success
2017-03-28 16:01:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1356,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 1, 21, 827845),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 8, 1, 18, 766913)}
2017-03-28 16:01:21 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:01:30 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:01:30 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:01:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:01:30 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:01:30 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:01:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:01:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:01:30 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:01:30 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:01:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:01:33 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/index.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 16:01:33 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:01:33 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:01:33 [heilongjiang] INFO: success
2017-03-28 16:01:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1314,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 1, 33, 601732),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 8, 1, 30, 560728)}
2017-03-28 16:01:33 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:11:37 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:11:37 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:11:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:11:37 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:11:37 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:11:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:11:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:11:38 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:11:38 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:11:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:11:38 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:11:38 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:11:38 [heilongjiang] INFO: success
2017-03-28 16:11:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 11, 38, 651680),
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'start_time': datetime.datetime(2017, 3, 28, 8, 11, 38, 377851)}
2017-03-28 16:11:38 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:14:20 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:14:20 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:14:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:14:20 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:14:20 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:14:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:14:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:14:20 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:14:20 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:14:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:14:52 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-28 16:14:59 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:14:59 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:14:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:14:59 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:14:59 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:14:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:14:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:15:00 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:15:00 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:15:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:16:10 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-28 16:16:14 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:16:14 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:16:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:16:14 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:16:14 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:16:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:16:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:16:14 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:16:14 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:16:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:16:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 http://www.12333sh.gov.cn/201412333/xxgk/zcgg/index_498.shtml>: HTTP status code is not handled or not allowed
2017-03-28 16:16:16 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 http://www.12333sh.gov.cn/201412333/xxgk/zcgg/index_499.shtml>: HTTP status code is not handled or not allowed
2017-03-28 16:16:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 http://www.12333sh.gov.cn/201412333/xxgk/zcgg/index_497.shtml>: HTTP status code is not handled or not allowed
2017-03-28 16:16:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 http://www.12333sh.gov.cn/201412333/xxgk/zcgg/index_496.shtml>: HTTP status code is not handled or not allowed
2017-03-28 16:16:49 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-28 16:19:08 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:19:08 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:19:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:19:08 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:19:08 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:19:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:19:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:19:08 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:19:08 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:19:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:19:09 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/index.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 16:19:09 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:19:09 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:19:09 [heilongjiang] INFO: success
2017-03-28 16:19:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1233,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 19, 9, 759766),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 8, 19, 8, 606362)}
2017-03-28 16:19:09 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:19:36 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:19:36 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:19:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:19:36 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:19:36 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:19:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:19:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:19:36 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:19:36 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:19:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:19:42 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-28 16:19:42 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-28 16:19:59 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:19:59 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:19:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:19:59 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:19:59 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:19:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:19:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:19:59 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:19:59 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:19:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:20:01 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/index.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 16:20:01 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:20:01 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:20:01 [heilongjiang] INFO: success
2017-03-28 16:20:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1242,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 20, 1, 983447),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 8, 19, 59, 787175)}
2017-03-28 16:20:01 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:21:28 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:21:28 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:21:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:21:28 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:21:28 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:21:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:21:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:21:29 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:21:29 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:21:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:21:30 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/index.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 16:21:30 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:21:30 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:21:30 [heilongjiang] INFO: success
2017-03-28 16:21:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1254,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 21, 30, 656652),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 8, 21, 29, 316192)}
2017-03-28 16:21:30 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:21:34 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:21:34 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:21:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:21:34 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:21:34 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:21:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:21:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:21:35 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:21:35 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:21:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:21:36 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/index.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 16:21:36 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:21:36 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:21:36 [heilongjiang] INFO: success
2017-03-28 16:21:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1254,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 21, 36, 596764),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 8, 21, 35, 297718)}
2017-03-28 16:21:36 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:24:31 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:24:31 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:24:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:24:31 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:24:31 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:24:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:24:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:24:32 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:24:32 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:24:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:24:33 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/index.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 16:24:33 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:24:33 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:24:33 [heilongjiang] INFO: success
2017-03-28 16:24:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1233,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 24, 33, 651905),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 8, 24, 32, 248075)}
2017-03-28 16:24:33 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:25:11 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:25:11 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:25:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:25:12 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:25:12 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:25:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:25:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:25:12 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:25:12 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:25:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:25:13 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/index.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 16:25:13 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:25:13 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:25:13 [heilongjiang] INFO: success
2017-03-28 16:25:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1254,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 25, 13, 581482),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 8, 25, 12, 339933)}
2017-03-28 16:25:13 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:25:17 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:25:17 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:25:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:25:17 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:25:17 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:25:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:25:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:25:17 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:25:17 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:25:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:25:18 [scrapy.core.scraper] ERROR: Error downloading <GET http://hl.lss.gov.cn/hljszc/index.jsp>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure builtins.ValueError: invalid literal for int() with base 10: b'0;'>]
2017-03-28 16:25:18 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:25:18 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:25:18 [heilongjiang] INFO: success
2017-03-28 16:25:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1248,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 25, 18, 498847),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 8, 25, 17, 354181)}
2017-03-28 16:25:18 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:26:01 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:26:01 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:26:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:26:01 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:26:01 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:26:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:26:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:26:01 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:26:01 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:26:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:26:13 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-28 16:26:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 362, in parse
    driver.get("http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court")
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 250, in get
    self.execute(Command.GET, {'url': url})
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 236, in execute
    response = self.command_executor.execute(driver_command, params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 415, in execute
    return self._request(command_info[0], url, body=data)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 489, in _request
    resp = opener.open(request, timeout=self._timeout)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 526, in open
    response = self._open(req, data)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 544, in _open
    '_open', req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 504, in _call_chain
    result = func(*args)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1346, in http_open
    return self.do_open(http.client.HTTPConnection, req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1321, in do_open
    r = h.getresponse()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 1331, in getresponse
    response.begin()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response
2017-03-28 16:26:13 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-28 16:26:13 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:26:13 [heilongjiang] INFO: success
2017-03-28 16:26:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 402,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30505,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 26, 13, 184544),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/RemoteDisconnected': 1,
 'start_time': datetime.datetime(2017, 3, 28, 8, 26, 1, 512196)}
2017-03-28 16:26:13 [scrapy.core.engine] INFO: Spider closed (shutdown)
2017-03-28 16:26:57 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:26:57 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:26:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:26:57 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:26:57 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:26:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:26:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:26:58 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:26:58 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:26:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:28:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 382, in parse
    driver.find_element_by_link_xpath(text_name).click()
AttributeError: 'WebDriver' object has no attribute 'find_element_by_link_xpath'
2017-03-28 16:28:55 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:28:55 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:28:55 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:28:55 [heilongjiang] INFO: success
2017-03-28 16:28:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 2,
 'downloader/request_bytes': 1200,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 30563,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 28, 55, 957143),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2017, 3, 28, 8, 26, 58, 174202)}
2017-03-28 16:28:55 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:43:01 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:43:01 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:43:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:43:02 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:43:02 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:43:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:43:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:43:02 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:43:02 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:43:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:43:03 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.baidu.com/>
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/twisted/internet/defer.py", line 1297, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/usr/local/lib/python3.6/site-packages/twisted/python/failure.py", line 389, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/usr/local/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 61: Connection refused.
2017-03-28 16:43:03 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:43:03 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:43:03 [heilongjiang] INFO: success
2017-03-28 16:43:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/request_bytes': 1185,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 43, 3, 329042),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2017, 3, 28, 8, 43, 2, 813566)}
2017-03-28 16:43:03 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:43:24 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:43:24 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:43:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:43:24 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:43:24 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:43:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:43:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:43:25 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:43:25 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:43:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:43:27 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-28 16:44:20 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:44:20 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:44:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:44:20 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:44:20 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:44:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:44:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:44:21 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:44:21 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:44:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:46:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 381, in parse
    links = driver.find_elements_by_xpath(hrefs)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 307, in find_elements_by_xpath
    return self.find_elements(by=By.XPATH, value=xpath)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 782, in find_elements
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 236, in execute
    response = self.command_executor.execute(driver_command, params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 412, in execute
    data = utils.dump_json(params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/utils.py", line 34, in dump_json
    return json.dumps(json_struct)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 180, in default
    o.__class__.__name__)
TypeError: Object of type '_Element' is not JSON serializable
2017-03-28 16:46:24 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:46:24 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:46:24 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:46:24 [heilongjiang] INFO: success
2017-03-28 16:46:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 402,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 29869,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 46, 24, 665628),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2017, 3, 28, 8, 44, 21, 700935)}
2017-03-28 16:46:24 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:48:16 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:48:16 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:48:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:48:16 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:48:16 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:48:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:48:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:48:17 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:48:17 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:48:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:49:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 381, in parse
    links = driver.find_elements_by_xpath(hrefs)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 307, in find_elements_by_xpath
    return self.find_elements(by=By.XPATH, value=xpath)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 782, in find_elements
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 236, in execute
    response = self.command_executor.execute(driver_command, params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 412, in execute
    data = utils.dump_json(params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/utils.py", line 34, in dump_json
    return json.dumps(json_struct)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 180, in default
    o.__class__.__name__)
TypeError: Object of type '_Element' is not JSON serializable
2017-03-28 16:49:28 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:49:28 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:49:28 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:49:28 [heilongjiang] INFO: success
2017-03-28 16:49:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 402,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30564,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 49, 28, 28551),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2017, 3, 28, 8, 48, 17, 389911)}
2017-03-28 16:49:28 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:49:55 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:49:55 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:49:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:49:56 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:49:56 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:49:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:49:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:49:56 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:49:56 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:49:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:52:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 381, in parse
    links = driver.find_elements_by_xpath(hrefs)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 307, in find_elements_by_xpath
    return self.find_elements(by=By.XPATH, value=xpath)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 782, in find_elements
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 236, in execute
    response = self.command_executor.execute(driver_command, params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 412, in execute
    data = utils.dump_json(params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/utils.py", line 34, in dump_json
    return json.dumps(json_struct)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 180, in default
    o.__class__.__name__)
TypeError: Object of type '_Element' is not JSON serializable
2017-03-28 16:52:30 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:52:30 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:52:30 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:52:30 [heilongjiang] INFO: success
2017-03-28 16:52:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 402,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30519,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 52, 30, 953332),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2017, 3, 28, 8, 49, 56, 729122)}
2017-03-28 16:52:30 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:53:24 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:53:24 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:53:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:53:25 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:53:25 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:53:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:53:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:53:25 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:53:25 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:53:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:53:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 381, in parse
    links = driver.find_elements_by_xpath(hrefs)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 307, in find_elements_by_xpath
    return self.find_elements(by=By.XPATH, value=xpath)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 782, in find_elements
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 236, in execute
    response = self.command_executor.execute(driver_command, params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 412, in execute
    data = utils.dump_json(params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/utils.py", line 34, in dump_json
    return json.dumps(json_struct)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 180, in default
    o.__class__.__name__)
TypeError: Object of type '_Element' is not JSON serializable
2017-03-28 16:53:38 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:53:38 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:53:38 [heilongjiang] INFO: success
2017-03-28 16:53:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 395,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30445,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 53, 38, 580842),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2017, 3, 28, 8, 53, 25, 795931)}
2017-03-28 16:53:38 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:53:55 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:53:55 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:53:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:53:55 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:53:55 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:53:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:53:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:53:56 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:53:56 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:53:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:54:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 381, in parse
    links = driver.find_elements_by_xpath(hrefs)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 307, in find_elements_by_xpath
    return self.find_elements(by=By.XPATH, value=xpath)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 782, in find_elements
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 236, in execute
    response = self.command_executor.execute(driver_command, params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 412, in execute
    data = utils.dump_json(params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/utils.py", line 34, in dump_json
    return json.dumps(json_struct)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/encoder.py", line 180, in default
    o.__class__.__name__)
TypeError: Object of type '_Element' is not JSON serializable
2017-03-28 16:54:23 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 16:54:23 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 16:54:23 [heilongjiang] INFO: success
2017-03-28 16:54:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 395,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30493,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 8, 54, 23, 733316),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2017, 3, 28, 8, 53, 56, 366569)}
2017-03-28 16:54:23 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 16:55:00 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 16:55:00 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 16:55:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 16:55:00 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:55:00 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 16:55:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 16:55:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 16:55:01 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 16:55:01 [scrapy.core.engine] INFO: Spider opened
2017-03-28 16:55:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 16:59:42 [heilongjiang] ERROR: parse_webpage_error:'lxml.etree._Element' object has no attribute 'url'
2017-03-28 17:00:22 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-28 17:00:36 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 17:00:36 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 17:00:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 17:00:37 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 17:00:37 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 17:00:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 17:00:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 17:00:37 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 17:00:37 [scrapy.core.engine] INFO: Spider opened
2017-03-28 17:00:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 17:01:25 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-28 17:01:38 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 17:01:38 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 17:01:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 17:01:38 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 17:01:38 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 17:01:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 17:01:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 17:01:39 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 17:01:39 [scrapy.core.engine] INFO: Spider opened
2017-03-28 17:01:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 17:03:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 390, in parse
    driver = driver.switchto.window(i)
AttributeError: 'WebDriver' object has no attribute 'switchto'
2017-03-28 17:03:28 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 17:03:28 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 17:03:28 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 17:03:28 [heilongjiang] INFO: success
2017-03-28 17:03:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 398,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30383,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 9, 3, 28, 223943),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2017, 3, 28, 9, 1, 39, 64651)}
2017-03-28 17:03:28 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 17:08:41 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 17:08:41 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 17:08:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 17:08:41 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 17:08:41 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 17:08:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 17:08:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 17:08:42 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 17:08:42 [scrapy.core.engine] INFO: Spider opened
2017-03-28 17:08:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 17:09:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 392, in parse
    page = driver.page_source
AttributeError: 'NoneType' object has no attribute 'page_source'
2017-03-28 17:09:24 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-28 17:09:24 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 17:09:24 [heilongjiang] INFO: success
2017-03-28 17:09:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 398,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 29884,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 28, 9, 9, 24, 317490),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2017, 3, 28, 9, 8, 42, 255459)}
2017-03-28 17:09:24 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-28 17:12:01 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 17:12:01 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 17:12:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 17:12:02 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 17:12:02 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 17:12:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 17:12:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 17:12:02 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 17:12:02 [scrapy.core.engine] INFO: Spider opened
2017-03-28 17:12:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 17:34:30 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-28 17:34:45 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 17:34:45 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 17:34:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 17:34:45 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 17:34:45 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 17:34:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 17:34:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 17:34:46 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 17:34:46 [scrapy.core.engine] INFO: Spider opened
2017-03-28 17:34:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 17:36:05 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-28 17:36:15 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-28 17:36:15 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-28 17:36:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-28 17:36:15 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 17:36:15 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-28 17:36:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-28 17:36:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-28 17:36:16 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-28 17:36:16 [scrapy.core.engine] INFO: Spider opened
2017-03-28 17:36:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 19:17:22 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-28 19:17:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 392, in parse
    page = driver.page_source
AttributeError: 'NoneType' object has no attribute 'page_source'
2017-03-28 19:17:30 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-28 19:17:30 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-28 19:17:30 [heilongjiang] INFO: get_webpage_count:0
2017-03-28 19:17:30 [heilongjiang] INFO: success
2017-03-28 19:17:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 398,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30458,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2017, 3, 28, 11, 17, 30, 12429),
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2017, 3, 28, 9, 36, 16, 599548)}
2017-03-28 19:17:30 [scrapy.core.engine] INFO: Spider closed (shutdown)
2017-03-29 10:17:33 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 10:17:33 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 10:17:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 10:17:33 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:17:33 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:17:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 10:17:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 10:17:33 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 10:17:33 [scrapy.core.engine] INFO: Spider opened
2017-03-29 10:17:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 10:18:30 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-29 10:19:32 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 10:19:32 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 10:19:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 10:19:32 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:19:32 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:19:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 10:19:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 10:19:33 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 10:19:33 [scrapy.core.engine] INFO: Spider opened
2017-03-29 10:19:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 10:19:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 392, in parse
    page = driver.page_source
AttributeError: 'NoneType' object has no attribute 'page_source'
2017-03-29 10:19:50 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-29 10:19:50 [heilongjiang] INFO: get_webpage_count:0
2017-03-29 10:19:50 [heilongjiang] INFO: success
2017-03-29 10:19:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 400,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30547,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 29, 2, 19, 50, 245748),
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2017, 3, 29, 2, 19, 33, 152062)}
2017-03-29 10:19:50 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-29 10:25:27 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 10:25:27 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 10:25:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 10:25:27 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:25:27 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:25:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 10:25:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 10:25:27 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 10:25:27 [scrapy.core.engine] INFO: Spider opened
2017-03-29 10:25:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 10:25:29 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-29 10:25:55 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 10:25:55 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 10:25:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 10:25:55 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:25:55 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:25:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 10:25:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 10:25:55 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 10:25:55 [scrapy.core.engine] INFO: Spider opened
2017-03-29 10:25:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 10:26:12 [heilongjiang] ERROR: parse_webpage_error:'lxml.etree._Element' object has no attribute 'url'
2017-03-29 10:26:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 382, in parse
    links[i].click()
IndexError: list index out of range
2017-03-29 10:26:15 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-29 10:26:15 [heilongjiang] INFO: get_webpage_count:0
2017-03-29 10:26:15 [heilongjiang] INFO: success
2017-03-29 10:26:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 404,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30508,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 29, 2, 26, 15, 351523),
 'log_count/ERROR': 2,
 'log_count/INFO': 9,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2017, 3, 29, 2, 25, 55, 760037)}
2017-03-29 10:26:15 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-29 10:27:01 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 10:27:01 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 10:27:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 10:27:01 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:27:01 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:27:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 10:27:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 10:27:01 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 10:27:01 [scrapy.core.engine] INFO: Spider opened
2017-03-29 10:27:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 10:41:36 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-29 10:41:51 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 10:41:51 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 10:41:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 10:41:51 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:41:51 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:41:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 10:41:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 10:41:52 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 10:41:52 [scrapy.core.engine] INFO: Spider opened
2017-03-29 10:41:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 10:44:25 [heilongjiang] ERROR: parse_webpage_error:'list' object has no attribute 'extract'
2017-03-29 10:44:33 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-29 10:45:23 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 10:45:23 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 10:45:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 10:45:23 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:45:23 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:45:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 10:45:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 10:45:23 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 10:45:23 [scrapy.core.engine] INFO: Spider opened
2017-03-29 10:45:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 10:51:07 [heilongjiang] ERROR: parse_webpage_error:sequence item 0: expected str instance, lxml.etree._Element found
2017-03-29 10:51:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 382, in parse
    links[i].click()
IndexError: list index out of range
2017-03-29 10:51:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 10:51:10 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-29 10:51:10 [heilongjiang] INFO: get_webpage_count:0
2017-03-29 10:51:10 [heilongjiang] INFO: success
2017-03-29 10:51:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 402,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 29871,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 29, 2, 51, 10, 864023),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2017, 3, 29, 2, 45, 23, 744316)}
2017-03-29 10:51:10 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-29 10:51:43 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 10:51:43 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 10:51:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 10:51:43 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:51:43 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:51:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 10:51:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 10:51:43 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 10:51:43 [scrapy.core.engine] INFO: Spider opened
2017-03-29 10:51:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 10:54:22 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-29 10:54:41 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 10:54:41 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 10:54:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 10:54:42 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:54:42 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:54:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 10:54:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 10:54:43 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 10:54:43 [scrapy.core.engine] INFO: Spider opened
2017-03-29 10:54:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 10:56:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 10:57:52 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-29 10:58:10 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 10:58:10 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 10:58:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 10:58:10 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:58:10 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 10:58:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 10:58:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 10:58:10 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 10:58:10 [scrapy.core.engine] INFO: Spider opened
2017-03-29 10:58:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 10:59:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 382, in parse
    links[i].click()
IndexError: list index out of range
2017-03-29 10:59:56 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 10:59:56 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-29 10:59:56 [heilongjiang] INFO: get_webpage_count:0
2017-03-29 10:59:56 [heilongjiang] INFO: success
2017-03-29 10:59:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 404,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30380,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 29, 2, 59, 56, 745827),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2017, 3, 29, 2, 58, 10, 827481)}
2017-03-29 10:59:56 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-29 11:04:27 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 11:04:27 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 11:04:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 11:04:27 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 11:04:27 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 11:04:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 11:04:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 11:04:27 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 11:04:27 [scrapy.core.engine] INFO: Spider opened
2017-03-29 11:04:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 11:05:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 11:08:20 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-29 11:08:39 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 11:08:39 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 11:08:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 11:08:39 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 11:08:39 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 11:08:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 11:08:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 11:08:40 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 11:08:40 [scrapy.core.engine] INFO: Spider opened
2017-03-29 11:08:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 11:13:44 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2017-03-29 11:16:13 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 2 items (at 1 items/min)
2017-03-29 11:17:28 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 3 items (at 1 items/min)
2017-03-29 11:17:33 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-29 11:17:37 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 11:17:37 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 11:17:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 11:17:37 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 11:17:37 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 11:17:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 11:17:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 11:17:37 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 11:17:37 [scrapy.core.engine] INFO: Spider opened
2017-03-29 11:17:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 11:17:40 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-29 11:17:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 362, in parse
    driver.get("http://hl.lss.gov.cn/hljszc/CtrlServlet?parm=search_Court")
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 250, in get
    self.execute(Command.GET, {'url': url})
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 236, in execute
    response = self.command_executor.execute(driver_command, params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 415, in execute
    return self._request(command_info[0], url, body=data)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 489, in _request
    resp = opener.open(request, timeout=self._timeout)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 526, in open
    response = self._open(req, data)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 544, in _open
    '_open', req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 504, in _call_chain
    result = func(*args)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1346, in http_open
    return self.do_open(http.client.HTTPConnection, req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1321, in do_open
    r = h.getresponse()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 1331, in getresponse
    response.begin()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response
2017-03-29 11:17:40 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-29 11:17:40 [heilongjiang] INFO: get_webpage_count:0
2017-03-29 11:17:40 [heilongjiang] INFO: success
2017-03-29 11:17:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 396,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30751,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2017, 3, 29, 3, 17, 40, 731497),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/RemoteDisconnected': 1,
 'start_time': datetime.datetime(2017, 3, 29, 3, 17, 37, 607449)}
2017-03-29 11:17:40 [scrapy.core.engine] INFO: Spider closed (shutdown)
2017-03-29 11:18:00 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 11:18:00 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 11:18:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 11:18:00 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 11:18:00 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 11:18:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 11:18:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 11:18:01 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 11:18:01 [scrapy.core.engine] INFO: Spider opened
2017-03-29 11:18:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 11:20:04 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 11 items (at 11 items/min)
2017-03-29 11:20:04 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-29 11:20:04 [heilongjiang] INFO: get_webpage_count:11
2017-03-29 11:20:04 [heilongjiang] INFO: success
2017-03-29 11:20:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 402,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30369,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 29, 3, 20, 4, 470400),
 'item_scraped_count': 11,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 3, 29, 3, 18, 1, 408413)}
2017-03-29 11:20:04 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-29 11:22:50 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 11:22:50 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 11:22:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 11:22:50 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 11:22:50 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 11:22:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 11:22:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 11:22:50 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 11:22:50 [scrapy.core.engine] INFO: Spider opened
2017-03-29 11:22:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 11:24:59 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 11:24:59 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-29 11:24:59 [heilongjiang] INFO: get_webpage_count:0
2017-03-29 11:24:59 [heilongjiang] INFO: success
2017-03-29 11:24:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 398,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 29927,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 29, 3, 24, 59, 377407),
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 3, 29, 3, 22, 50, 673496)}
2017-03-29 11:24:59 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-29 11:27:34 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 11:27:34 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 11:27:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 11:27:34 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 11:27:34 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 11:27:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 11:27:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 11:27:34 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 11:27:34 [scrapy.core.engine] INFO: Spider opened
2017-03-29 11:27:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 11:27:35 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-29 11:27:48 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 11:27:48 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 11:27:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 11:27:48 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 11:27:48 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 11:27:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 11:27:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 11:27:49 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 11:27:49 [scrapy.core.engine] INFO: Spider opened
2017-03-29 11:27:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 11:29:50 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 11:29:50 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-29 11:29:50 [heilongjiang] INFO: get_webpage_count:0
2017-03-29 11:29:50 [heilongjiang] INFO: success
2017-03-29 11:29:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 402,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 29868,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 29, 3, 29, 50, 939780),
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 3, 29, 3, 27, 49, 267749)}
2017-03-29 11:29:50 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-29 13:15:10 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 13:15:10 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 13:15:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 13:15:10 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 13:15:10 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 13:15:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 13:15:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 13:15:10 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 13:15:10 [scrapy.core.engine] INFO: Spider opened
2017-03-29 13:15:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 13:15:17 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-29 13:15:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 361, in parse
    driver = webdriver.PhantomJS()
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/phantomjs/webdriver.py", line 52, in __init__
    self.service.start()
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/common/service.py", line 96, in start
    self.assert_process_still_running()
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/common/service.py", line 109, in assert_process_still_running
    % (self.path, return_code)
selenium.common.exceptions.WebDriverException: Message: Service phantomjs unexpectedly exited. Status code was: -2

2017-03-29 13:15:17 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-29 13:15:18 [heilongjiang] INFO: get_webpage_count:0
2017-03-29 13:15:18 [heilongjiang] INFO: success
2017-03-29 13:15:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 395,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30393,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2017, 3, 29, 5, 15, 18, 3966),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/WebDriverException': 1,
 'start_time': datetime.datetime(2017, 3, 29, 5, 15, 10, 465955)}
2017-03-29 13:15:18 [scrapy.core.engine] INFO: Spider closed (shutdown)
2017-03-29 13:15:24 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 13:15:24 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 13:15:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 13:15:24 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 13:15:24 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 13:15:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 13:15:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 13:15:24 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 13:15:24 [scrapy.core.engine] INFO: Spider opened
2017-03-29 13:15:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 13:17:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 375, in parse
    driver.find_element_by_xpath(next_page).click()  # 数据由js来控制,点击后加载数据
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 295, in find_element_by_xpath
    return self.find_element(by=By.XPATH, value=xpath)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 756, in find_element
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 238, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 193, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: {"errorMessage":"Unable to find element with xpath '//li[@class=\"page-txt\"][3]/a'","request":{"headers":{"Accept":"application/json","Accept-Encoding":"identity","Connection":"close","Content-Length":"114","Content-Type":"application/json;charset=UTF-8","Host":"127.0.0.1:55701","User-Agent":"Python-urllib/3.6"},"httpVersion":"1.1","method":"POST","post":"{\"using\": \"xpath\", \"value\": \"//li[@class=\\\"page-txt\\\"][3]/a\", \"sessionId\": \"b89c7a40-143e-11e7-850b-a353922ec107\"}","url":"/element","urlParsed":{"anchor":"","query":"","file":"element","directory":"/","path":"/element","relative":"/element","port":"","host":"","password":"","user":"","userInfo":"","authority":"","protocol":"","source":"/element","queryKey":{},"chunks":["element"]},"urlOriginal":"/session/b89c7a40-143e-11e7-850b-a353922ec107/element"}}
Screenshot: available via screen

2017-03-29 13:17:43 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 13:17:43 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-29 13:17:43 [heilongjiang] INFO: get_webpage_count:0
2017-03-29 13:17:43 [heilongjiang] INFO: success
2017-03-29 13:17:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 402,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30514,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 29, 5, 17, 43, 658742),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchElementException': 1,
 'start_time': datetime.datetime(2017, 3, 29, 5, 15, 24, 873068)}
2017-03-29 13:17:43 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-29 13:21:50 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 13:21:50 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 13:21:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 13:21:50 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 13:21:50 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 13:21:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 13:21:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 13:21:51 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 13:21:51 [scrapy.core.engine] INFO: Spider opened
2017-03-29 13:21:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 13:33:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 374, in parse
    next_page = driver.find_element_by_xpath('//li[@class="page-txt"][3]/a')
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 295, in find_element_by_xpath
    return self.find_element(by=By.XPATH, value=xpath)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 756, in find_element
    'value': value})['value']
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 238, in execute
    self.error_handler.check_response(response)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py", line 193, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: {"errorMessage":"Unable to find element with xpath '//li[@class=\"page-txt\"][3]/a'","request":{"headers":{"Accept":"application/json","Accept-Encoding":"identity","Connection":"close","Content-Length":"114","Content-Type":"application/json;charset=UTF-8","Host":"127.0.0.1:57338","User-Agent":"Python-urllib/3.6"},"httpVersion":"1.1","method":"POST","post":"{\"using\": \"xpath\", \"value\": \"//li[@class=\\\"page-txt\\\"][3]/a\", \"sessionId\": \"a0c88340-143f-11e7-a8b8-d91017340b4f\"}","url":"/element","urlParsed":{"anchor":"","query":"","file":"element","directory":"/","path":"/element","relative":"/element","port":"","host":"","password":"","user":"","userInfo":"","authority":"","protocol":"","source":"/element","queryKey":{},"chunks":["element"]},"urlOriginal":"/session/a0c88340-143f-11e7-a8b8-d91017340b4f/element"}}
Screenshot: available via screen

2017-03-29 13:33:19 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 13:33:19 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-29 13:33:19 [heilongjiang] INFO: get_webpage_count:0
2017-03-29 13:33:19 [heilongjiang] INFO: success
2017-03-29 13:33:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 395,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30436,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 29, 5, 33, 19, 600557),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/NoSuchElementException': 1,
 'start_time': datetime.datetime(2017, 3, 29, 5, 21, 51, 82480)}
2017-03-29 13:33:19 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-29 13:34:21 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 13:34:21 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 13:34:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 13:34:21 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 13:34:21 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 13:34:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 13:34:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 13:34:21 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 13:34:21 [scrapy.core.engine] INFO: Spider opened
2017-03-29 13:34:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 13:38:56 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2017-03-29 13:39:24 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 8 items (at 7 items/min)
2017-03-29 13:40:21 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 20 items (at 12 items/min)
2017-03-29 13:40:30 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-29 13:40:30 [heilongjiang] INFO: get_webpage_count:22
2017-03-29 13:40:30 [heilongjiang] INFO: success
2017-03-29 13:40:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 400,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 29903,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 29, 5, 40, 30, 684438),
 'item_scraped_count': 22,
 'log_count/INFO': 12,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 3, 29, 5, 34, 21, 807335)}
2017-03-29 13:40:30 [scrapy.core.engine] INFO: Spider closed (finished)
2017-03-29 13:50:12 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 13:50:12 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 13:50:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 13:50:12 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 13:50:12 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 13:50:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 13:50:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 13:50:12 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 13:50:12 [scrapy.core.engine] INFO: Spider opened
2017-03-29 13:50:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 13:51:00 [scrapy.crawler] INFO: Received SIG_UNBLOCK, shutting down gracefully. Send again to force 
2017-03-29 13:51:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.baidu.com/> (referer: None)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/local/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiaxiaodong/python/python3/tribunal_spider/tribunal_spider/spiders/spider.py", line 387, in parse
    links[i].click()
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webelement.py", line 77, in click
    self._execute(Command.CLICK_ELEMENT)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webelement.py", line 491, in _execute
    return self._parent.execute(command, params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py", line 236, in execute
    response = self.command_executor.execute(driver_command, params)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 415, in execute
    return self._request(command_info[0], url, body=data)
  File "/usr/local/lib/python3.6/site-packages/selenium/webdriver/remote/remote_connection.py", line 489, in _request
    resp = opener.open(request, timeout=self._timeout)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 526, in open
    response = self._open(req, data)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 544, in _open
    '_open', req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 504, in _call_chain
    result = func(*args)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1346, in http_open
    return self.do_open(http.client.HTTPConnection, req)
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py", line 1321, in do_open
    r = h.getresponse()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 1331, in getresponse
    response.begin()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response
2017-03-29 13:51:00 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-03-29 13:51:00 [heilongjiang] INFO: get_webpage_count:0
2017-03-29 13:51:00 [heilongjiang] INFO: success
2017-03-29 13:51:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 396,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 30451,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2017, 3, 29, 5, 51, 0, 570456),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/RemoteDisconnected': 1,
 'start_time': datetime.datetime(2017, 3, 29, 5, 50, 12, 831405)}
2017-03-29 13:51:00 [scrapy.core.engine] INFO: Spider closed (shutdown)
2017-03-29 13:51:21 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: tribunal_spider)
2017-03-29 13:51:21 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tribunal_spider', 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'tribunal_spider.spiders', 'SPIDER_MODULES': ['tribunal_spider.spiders']}
2017-03-29 13:51:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-03-29 13:51:21 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 13:51:21 [py.warnings] WARNING: /usr/local/lib/python3.6/site-packages/scrapy/utils/deprecate.py:156: ScrapyDeprecationWarning: `scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware` class is deprecated, use `scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware` instead
  ScrapyDeprecationWarning)

2017-03-29 13:51:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['tribunal_spider.midle.ProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'tribunal_spider.midle.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-29 13:51:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-29 13:51:22 [scrapy.middleware] INFO: Enabled item pipelines:
['tribunal_spider.pipelines.TribunalSpiderPipeline']
2017-03-29 13:51:22 [scrapy.core.engine] INFO: Spider opened
2017-03-29 13:51:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-29 13:53:31 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2017-03-29 13:54:23 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 15 items (at 14 items/min)
2017-03-29 13:55:25 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 31 items (at 16 items/min)
2017-03-29 13:56:25 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 47 items (at 16 items/min)
2017-03-29 13:57:23 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 62 items (at 15 items/min)
2017-03-29 13:58:23 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 78 items (at 16 items/min)
2017-03-29 13:59:25 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 92 items (at 14 items/min)
2017-03-29 14:00:22 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 105 items (at 13 items/min)
2017-03-29 14:01:22 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 120 items (at 15 items/min)
2017-03-29 14:02:24 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 135 items (at 15 items/min)
2017-03-29 14:03:23 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 150 items (at 15 items/min)
2017-03-29 14:04:27 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 166 items (at 16 items/min)
2017-03-29 14:05:23 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 177 items (at 11 items/min)
2017-03-29 14:06:23 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 192 items (at 15 items/min)
2017-03-29 14:07:23 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 208 items (at 16 items/min)
2017-03-29 14:08:23 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 224 items (at 16 items/min)
2017-03-29 14:09:23 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 236 items (at 12 items/min)
2017-03-29 14:11:24 [heilongjiang] ERROR: save_time_error:time data '' does not match format '%Y年%m月%d日'
2017-03-29 14:12:45 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 249 items (at 13 items/min)
2017-03-29 14:13:25 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 259 items (at 10 items/min)
2017-03-29 14:14:24 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 274 items (at 15 items/min)
2017-03-29 14:15:22 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 289 items (at 15 items/min)
2017-03-29 14:16:22 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 305 items (at 16 items/min)
2017-03-29 14:17:22 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 321 items (at 16 items/min)
2017-03-29 14:18:25 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 338 items (at 17 items/min)
2017-03-29 14:19:25 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 354 items (at 16 items/min)
2017-03-29 14:20:23 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 370 items (at 16 items/min)
2017-03-29 14:21:23 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 386 items (at 16 items/min)
2017-03-29 14:22:25 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 403 items (at 17 items/min)
2017-03-29 14:23:26 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 419 items (at 16 items/min)
2017-03-29 14:24:24 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 435 items (at 16 items/min)
2017-03-29 14:25:23 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 451 items (at 16 items/min)
2017-03-29 14:26:25 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 467 items (at 16 items/min)
2017-03-29 14:27:23 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 482 items (at 15 items/min)
2017-03-29 14:28:23 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 498 items (at 16 items/min)
2017-03-29 14:29:22 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 514 items (at 16 items/min)
2017-03-29 14:30:22 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 530 items (at 16 items/min)
2017-03-29 14:30:59 [scrapy.core.engine] INFO: Closing spider (finished)
2017-03-29 14:30:59 [heilongjiang] INFO: get_webpage_count:530
2017-03-29 14:30:59 [heilongjiang] INFO: success
2017-03-29 14:30:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 404,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 29878,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 29, 6, 30, 59, 181726),
 'item_scraped_count': 530,
 'log_count/ERROR': 1,
 'log_count/INFO': 45,
 'log_count/WARNING': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2017, 3, 29, 5, 51, 22, 109593)}
2017-03-29 14:30:59 [scrapy.core.engine] INFO: Spider closed (finished)
